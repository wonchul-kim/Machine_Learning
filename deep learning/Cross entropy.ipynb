{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why negative log-likelihood?\n",
    "\n",
    "딥러닝 모델의 손실함수로 음의 로그우도(negative log-likelihood)가 쓰입니다. 어떤 이유에서일까요?\n",
    "\n",
    "#### 확률론적 접근\n",
    "\n",
    "딥러닝 모델을 학습시키기 위해 최대우도추정(Maximum Likelihood Estimation) 기법을 씁니다. 주어진 데이터만으로 미지의 최적 모델 파라메터 θ를 찾아야 합니다. 입력값 X와 파라메터 θ가 주어졌을 때 정답 Y가 나타날 확률, 즉 우도 P(Y|X;θ)를 최대화하는 θ가 바로 우리가 찾고 싶은 결과라고 보면 되겠습니다.\n",
    "\n",
    "그런데 학습데이터 각각의 우도를 스케일해도 전체 argmax의 결과는 바뀌지 않으므로 ‘우도의 곱을 최대’로 만드는 θ와 ‘로그우도의 기대값, 즉 ΣxP(y|x)logP(y|x;θ)를 최대’로 하는 θ는 같습니다. 이와 관련해 Deep Learning Book 128페이지에는 다음과 같이 설명돼 있습니다.\n",
    "\n",
    "The argmax does not change when we rescale the cost function, we can divide by the total number of data to obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution Pdata defined by the training data.\n",
    "\n",
    "다범주 분류를 학습하는 딥러닝 모델의 경우 말단에 다음과 같이 소프트맥스 함수가 적용됩니다. (f(x)는 소프트맥스 계층의 입력값)\n",
    "\n",
    "P(yi|xi;θ)=exp{f(xi)}∑jexp{f(xj)}\n",
    "위 식에서 f는 범주 수만큼의 차원을 갖는 벡터로써 unnormalized log probabilities에 해당합니다. 소프트맥스 함수가 취해짐으로써 그 요소의 합이 1이 됩니다. 정답 인덱스에 해당하는 f의 요소값을 높인다는 말은 우도를 높인다(=입력값 X를 넣었을 때 Y 관련 스코어를 높인다)는 의미로 해석할 수 있습니다. 최대우도추정과 관련해서는 이곳을 참고하시면 좋을 것 같습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
