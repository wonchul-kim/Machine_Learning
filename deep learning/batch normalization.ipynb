{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://excelsior-cjh.tistory.com/178\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization은 기본적으로 Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이다. 지금까지는 이 문제를 Activation 함수의 변화 (ReLU 등), Careful Initialization, small learning rate 등으로 해결하였지만, 이 논문에서는 이러한 간접적인 방법보다는 training 하는 과정 자체를 전체적으로 안정화하여 학습 속도를 가속시킬 수 있는 근본적인 방법을 찾고싶어 했다.\n",
    "\n",
    "이들은 이러한 불안정화가 일어나는 이유가 ‘Internal Covariance Shift’ 라고 주장하고 있다. **Internal Covariance Shift라는 현상은 Network의 각 층이나 Activation 마다 input의 distribution이 달라지는 현상을 의미한다.** 이 현상을 막기 위해서 간단하게 각 층의 input의 distribution을 평균 0, 표준편차 1인 input으로 normalize 시키는 방법을 생각해볼 수 있고, 이는 whitening이라는 방법으로 해결할 수 있다. Whitening은 기본적으로 들어오는 input의 feature들을 uncorrelated 하게 만들어주고, 각각의 variance를 1로 만들어주는 작업이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제는 whitening을 하기 위해서는 covariance matrix의 계산과 inverse의 계산이 필요하기 때문에 계산량이 많을 뿐더러, 설상가상으로 whitening을 하면 일부 parameter 들의 영향이 무시된다는 것이다. \n",
    "\n",
    "예를 들어 input $u$를 받아 $x = u + b$라는 output을 내놓고 적절한 bias $b$를 학습하려는 네트워크에서 $x$에 $\\mathbb{E}[x]$를 빼주는 작업을 한다고 생각해보자. 그럴 경우 $\\mathbb{E}[x]$를 빼는 과정에서 $b$의 값이 같이 빠지고, 결국 output에서 $b$의 영향은 없어지고 만다. \n",
    "\n",
    "단순히 $\\mathbb{E}[x]$를 빼는 것이 아니라 표준편차로 나눠주는 등의 scaling 과정까지 들어갈 경우 이러한 경향은 더욱 악화될 것이고, 논문에서는 이를 실험적으로 확인했다고 한다.\n",
    "\n",
    "이와 같은 whitening의 단점을 보완하고, internal covariance shift는 줄이기 위해 논문에서는 다음과 같은 접근을 취했다.\n",
    "\n",
    "각각의 feature들이 이미 uncorrelated 되어있다고 가정하고, feature 각각에 대해서만 scalar 형태로 mean과 variance를 구하고 각각 normalize 한다.\n",
    "단순히 mean과 variance를 0, 1로 고정시키는 것은 오히려 Activation function의 nonlinearity를 없앨 수 있다. 예를 들어 sigmoid activation의 입력이 평균 0, 분산 1이라면 출력 부분은 곡선보다는 직선 형태에 가까울 것이다. 또한, ***feature가 uncorrelated 되어있다는 가정에 의해 네트워크가 표현할 수 있는 것이 제한될 수 있다. 이 점들을 보완하기 위해, normalize된 값들에 scale factor (gamma)와 shift factor (beta)를 더해주고 이 변수들을 back-prop 과정에서 같이 train 시켜준다.***\n",
    "\n",
    "training data 전체에 대해 mean과 variance를 구하는 것이 아니라, mini-batch 단위로 접근하여 계산한다. 현재 택한 mini-batch 안에서만 mean과 variance를 구해서, 이 값을 이용해서 normalize 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "뉴럴넷을 학습시킬 때 보통 mini-batch 단위로 데이터를 가져와서 학습을 시키는데, 각 feature 별로 평균과 표준편차를 구해준 다음 normalize 해주고, scale factor와 shift factor를 이용하여 새로운 값을 만들어준다. \n",
    "\n",
    "<img src='./imgs/bn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 이 Batch Normalization을 네트워크에 적용시킬 때는, 특정 Hidden Layer에 들어가기 전에 Batch Normalization Layer를 더해주어 input을 modify해준 뒤 새로운 값을 activation function으로 넣어주는 방식으로 사용한다.\n",
    "\n",
    "Training Data로 학습을 시킬 때는 현재 보고있는 mini-batch에서 평균과 표준편차를 구하지만, ***Test Data를 사용하여 Inference를 할 때는 다소 다른 방법을 사용한다. mini-batch의 값들을 이용하는 대신 지금까지 본 전체 데이터를 다 사용한다는 느낌으로,  training 할 때 현재까지 본 input들의 이동평균 (moving average) 및 unbiased variance estimate의 이동평균을 계산하여 저장해놓은 뒤 이 값으로 normalize를 한다. 마지막에 gamma와 beta를 이용하여 scale/shift 해주는 것은 동일하다.***\n",
    "\n",
    "전체적인 Batch Normalization layer의 pseudo-code 는 다음과 같다. 논문에는 다소 헷갈릴 수 있는 방식으로 설명이 되어있는데, 결국 중요한 것은 ‘Training 할 때는 mini-batch의 평균과 분산으로 normalize 하고, Test 할 때는 계산해놓은 이동 평균으로 normalize 한다. Normalize 한 이후에는 scale factor와 shift factor를 이용하여 새로운 값을 만들고, 이 값을 내놓는다. 이 Scale factor와 Shift factor는 다른 레이어에서 weight를 학습하듯이 back-prop에서 학습하면 된다.’ 라는 흐름이다.\n",
    "\n",
    "<img src='./imgs/bn2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단, 지금까지의 설명은 ‘일반적인 Network 일 때’ 에 한해서 통용된다. ***만약 Batch Normalization을 CNN에 적용시키고 싶을 경우 지금까지 설명한 방법과는 다소 다른 방법을 이용해야만 한다. 먼저, convolution layer에서 보통 activation function에 값을 넣기 전 $Wx + b$ 형태로 weight를 적용시키는데, Batch Normalization을 사용하고 싶을 경우 normalize 할 때 beta 값이 b의 역할을 대체할 수 있기 때문에 b를 없애준다. 또한, CNN의 경우 convolution의 성질을 유지시키고 싶기 때문에, 각 channel을 기준으로 각각의 Batch Normalization 변수들을 만든다. 예를 들어 m의 mini-batch-size, n의 channel size 를 가진 Convolution Layer에서 Batch Normalization을 적용시킨다고 해보자. convolution을 적용한 후의 feature map의 사이즈가 $p x q$ 일 경우, 각 채널에 대해 $m x p x q$ 개의 각각의 스칼라 값에 대해 mean과 variance를 구하는 것이다. 최종적으로 gamma와 beta는 각 채널에 대해 한개씩 해서 총 $m$개의 독립적인 Batch Normalization 변수들이 생기게 된다.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefit\n",
    "\n",
    "논문에서 주장하는 Batch Normalization의 장점은 다음과 같다.\n",
    "\n",
    "1. 기존 Deep Network에서는 learning rate를 너무 높게 잡을 경우 gradient가 explode/vanish 하거나, 나쁜 local minima에 빠지는 문제가 있었다. 이는 parameter들의 scale 때문인데, Batch Normalization을 사용할 경우 propagation 할 때 parameter의 scale에 영향을 받지 않게 된다. 따라서, learning rate를 크게 잡을 수 있게 되고 이는 빠른 학습을 가능케 한다.\n",
    "\n",
    "2. Batch Normalization의 경우 자체적인 regularization 효과가 있다. 이는 기존에 사용하던 weight regularization term 등을 제외할 수 있게 하며, 나아가 Dropout을 제외할 수 있게 한다 (Dropout의 효과와 Batch Normalization의 효과가 같기 때문.) . Dropout의 경우 효과는 좋지만 학습 속도가 다소 느려진다는 단점이 있는데, 이를 제거함으로서 학습 속도도 향상된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization 논문에서는 SGD가 다음과 같은 특징들이 있다고 정리하고 있다.\n",
    "\n",
    "* Stochastic Gradient는 실제 Gradient의 추정값이며 이것은 미니배치의 크기가 이 커질수록 더 정확한 추정값을 가지게 된다.\n",
    "* 미니배치를 뽑아서 연산을 수행하기 때문에 최신 컴퓨팅 플랫폼에 의하여 병렬적인 연산 수행이 가능하여 더욱 효율적이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD의 문제점은 뉴럴넷을 학습하기 위한 하이퍼 파라미터들의 초기값 설정을 굉장히 신중하게 해줘야만 한다는 점이다. 이전 네트워크에서의 파라미터 변화는 그 다음, 그 다음 네트워크들을 거치며 점점 변화량이 증폭된다. 따라서 신중하지 못한 파라미터 초기화의 문제점은 레이어가 쌓이면서 더욱 큰 문제로 발전하게 될 것이다.\n",
    "\n",
    "뉴럴넷의 입력 분포의 변화가 일어난다면 뉴럴넷의 각 레이어들은 그 새로운 분포에 끊임없이 적응해야하는 문제가 있다. 이 경우에 뉴럴넷은 Covariate Shift를 겪었다고 말한다.\n",
    "\n",
    "다음과 같은 뉴럴넷 연산을 살펴보자.\n",
    "\n",
    "$$ l = F_2(F_1(u, \\theta_1), \\theta_2) $$\n",
    "\n",
    "\n",
    "$F_1$, $F_2$는 임의의 Transformation이고, 파라미터 $\\theta_1$, $\\theta_2$는 Loss 을 최소화하는 방향으로 학습이 진행된다.\n",
    "\n",
    "여기서 하위 네트워크 $F_2$의 파라미터 $\\theta_2$는 다음과 같은 방식으로 학습이 된다고 볼 수 있다.\n",
    "\n",
    "$$ \\theta_2 = \\theta_2 - \\frac{\\alpha}{m}\\sum_{i=1}^m \\nabla_{\\theta_2}F_2(x_i, \\theta_2)$$\n",
    " \n",
    " \n",
    "이건 마치 $F_2(x, \\theta_2)$를 단독으로 학습하는 것과 동일하다. 따라서 학습을 더 원할하게 만들어주는 입력 분포는 모델을 구성하는 하위 네트워크들에 대해서도 동일하게 적용된다고 볼 수 있다!\n",
    "\n",
    "여기까지 나온 개념을 일단 정리해 보자면:\n",
    "\n",
    "* 뉴럴넷의 입력 분포 변화는 일어나지 않는 것이 좋다! 왜? 뉴럴넷 파라미터들이 그 분포에 새로 적응해야 하기 때문!\n",
    "* 즉, 학습을 원할하게 하기 위해 효율적인 입력 분포는 일정하게 유지되는 입력 분포\n",
    "* 네트워크를 여러 층을 쌓은 경우에 하위 네트워크에 대한 입력 분포의 효율성에도 동일하게 적용된다!\n",
    "* 즉, 앞단의 레이어의 출력(다시말해 뒷단의 레이어의 입력이 될 값)의 분포도 일정하게 유지하는 것이 좋음!\n",
    "\n",
    "논문에서 언급하고있는 또 다른 문제점은 Gradient Descent 및 뉴럴넷의 Nonlinearity에 의한 Saturated Regime에 관한 문제이다. 일반적으로 뉴럴넷의 Activation은 Sigmoid를 사용하는데 이 Sigmoid의 특성상 절대값이 일정 수준 이상으로 큰 입력에 대해서는 Gradient가 거의 사라지는 문제가 발생한다.\n",
    "\n",
    "즉, 다음의 뉴럴넷 구조를 살펴보자.\n",
    "\n",
    "$$ z = g(Wu + b)$$\n",
    "\n",
    "여기서 g는 다음과 같은 Sigmoid 함수이다.\n",
    "\n",
    "$$ g = \\frac{1}{1 + e^{-x}}$$\n",
    " \n",
    "이때 Sigmoid 함수의 특성상 입력 $x$의 절대값의 크기가 커질수록, 즉 0으로부터 멀어질수록 Gradient 값이 매우 작아지게 된다. 따라서 0 근방이 아닌 입력들에 대해서는 잘 학습이 되지 않을 것이며 만약 $x$의 분포가 0으로부터 멀어진다면 더이상 학습이 진행이 되지 않고 모델은 특정 파라미터 값에서 Saturation이 되게 될 것이다. 이 현상을 Saturated Regime에 빠졌다고 하는 것 같다. Saturated Regime에 대해서는 좀 더 조사해보고 정리하도록 하겠다\n",
    "\n",
    "어쨌든 이러한 이유들이 뉴럴넷 모델의 각 레이어들의 입력들의 분포를 일정하게 유지시켜주는 것이 왜 필요한지에 대한 설명이 될 것 같다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariate Shift를 줄이기 위한 시도\n",
    "\n",
    "* Internal Covariate Shift의 정의: 네트워크의 학습 도중에 파라미터의 변화로 인한 네트워크 Activation(출력)들의 분포 변화\n",
    "\n",
    "학습 효율을 높이기 위해서는 이런 Internal Covariate Shift를 줄이기 위한 노력이 필요하다. 기존의 여러 연구 결과들은 입력값들이 Whitening된다면, 즉, Zero Mean과 Unit Variance를 가지게 되고 각각의 입력값들이 Decorrelated된다면, 뉴럴넷이 훨씬 빠르게 수렴할 것이라고 말하고 있다. 또한 모든 레이어들이 같은 Whitening 방식을 공유한다면 훨씬 이득을 가질 수 있다고 한다.\n",
    "\n",
    "기존의 연구에서는 뉴럴넷의 파라미터를 Activation의 값에 따라서 바꾸면서 Whitening하는 방식을 사용하였다. 하지만 이러한 방법은 Gradient Descent Step의 효과를 줄이는 결과를 가져온다.\n",
    "\n",
    "예를 들면, 입력값 $u$에 학습된 바이어스 $b$를 더해주고 트레이닝용 데이터의 Activation의 Mean을 빼주는 방식으로 Normalization을 수행하는 레이어를 생각해보자.\n",
    "\n",
    "$$ \\hat{x} \\gets x - \\mathbb{E}[x], \\quad where \\, x = u + b, X = \\{x_1, x_2, ..., x_N\\}$$\n",
    "\n",
    "만약 Gradient Descent Step이 $\\mathbb{E}[x]$와 의 Dependency를 무시하고 수행된다면 다음과 같은 업데이트 룰을 따르게 된다.\n",
    "\n",
    "$$ b \\gets b + \\nabla b, \\quad where \\, \\nabla b \\propto -\\frac{\\partial l}{\\partial \\hat{x}}$$\n",
    "이러한 업데이트 룰을 따르게 된다면 다음과 같은 결과를 확인할 수 있다.\n",
    "\n",
    "$$ u + (b + \\nabla b) - \\mathbb{E}[u + (b + \\nabla b)] = u + b - \\mathbb{E}[u + b]$$\n",
    "\n",
    "즉, 업데이트 전후의 Normalization 결과 $\\hat{x}$가 같게 되며 결과적으로 Loss $l$의 값 역시 일정하게 유지된다. $\\hat{x}$, $l$의 변화가 없게 된다면 Gradient $\\nabla b$의 값이 불분명하게 되고 이것은 업데이트가 제대로 되지 않는 결과를 가져오게 된다. 이러한 현상은 단순히 Zero Mean을 위한 Centering 뿐 아니라 Unit Variance를 위한 Scaling에서도 마찬가지로 발생되게 된다. 따라서 이러한 방식의 Normalization은 문제가 된다!\n",
    "\n",
    "위의 현상을 다음과 같이 설명할 수 있다. 먼저 Normalization을 다음과 같이 표현해보자.\n",
    "\n",
    "$$ \\hat{x} = Norm(x, X)$$\n",
    "\n",
    "따라서 이 Normalization 레이어의 Backpropagation을 구하려면 다음의 Jacobian을 구해야 한다.\n",
    "\n",
    "$$ \\frac{\\partial Norm(x, X)}{\\partial x}, \\, and \\, \\frac{\\partial Norm(x, X)}{\\partial X}$$ \n",
    " \n",
    "만약 $\\mathbb{E}$와 $b$의 Dependency를 무시한다는 것은 즉 뒤의 Term인 $\\frac{\\partial Norm(x, X)}{\\partial X}$을 무시한다는 것이다. 즉, $\\hat{x}$은 엄밀하게는 다음과 같이 표현해야 한다.\n",
    "\n",
    "$$ \\hat{x} = Norm(x, X) = x - \\mathbb{E}_{x \\in X}[x], \\quad where x = u + b, \\quad X = {x_1, ..., x_N}$$\n",
    "\n",
    "추가적으로 이러한 방식의 Whitening은 Covariance를 구해야 한다는 이유로 인하여 연산이 매우 복잡하다는 단점이 있다. Covarialce Matrix를 구하려면 다음의 연산을 수행하여야 한다.\n",
    "\n",
    "$$Cov[x] = \\mathbb{E}_{x \\in X}[xx^T] - \\mathbb{E}[x]\\mathbb{E}[x]^T$$\n",
    "$$ Cov[x]^{-\\frac{1}{2}}(x - \\mathbb{E}[x])$$\n",
    "\n",
    "뿐만아니라 이것들의 Backpropagation까지 구해야 한다!\n",
    "\n",
    "따라서 Batch Normalization의 저자들은 여기에서 동기를 얻어서 파라미터 업데이트 이후마다의 트레이닝 셋 전체의 분석이 필요하지 않을 뿐 아니라 미분도 가능해서 Backpropagation을 구하는 것이 용이한 어떤 입력 Normalization을 찾는 연구를 시도했다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: 미니배치의 Statistics를 이용\n",
    "\n",
    "Batch Normalization은 각각의 스칼라 Feature들을 독립적으로 정규화하는 방식으로 진행된다. 즉, 각각의 Feature들의 Mean 및 Variance를 0과 1로 정규화를 하는 것이다. 정규화를 위해서는 $d$차원의 입력 $x = (x^{(1)}, ..., x^{(d)})$에 대해서 다음의 연산을 수행해야 한다.\n",
    "\n",
    "$$ \\hat{x}^{(k)} = \\frac{x^{(k)} - \\mathbb{E}[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$$\n",
    " \n",
    "근데 위에서 설명하였듯이 저런식으로 하면 문제가 발생한다. 따라서 여기서는 각각의 Activation 에 대해서 새로운 파라미터 쌍 $\\gamma^{(k)}$, $\\beta^{(k)}$ 을 도입하여 이 문제를 해결한다.\n",
    "\n",
    "$$ yY{(k)} = \\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)}$$\n",
    "\n",
    "이 파라미터 $\\gamma^{(k)}$와 $\\beta^{(k)}$는 모델이 학습되어감에 따라 함께 학습이 이루어지며 이 파라미터의 역할은 본래 모델의 Representation Power을 유지시키는 역할을 하며 이 Representation Power 덕분에 위에서 언급했었던 단순 정규화의 문제점이 해결이 된다. 만약 다음과 같이 세팅을 할 수 있다면(Optimal하게 학습이 이루어졌다고 가졍한다면) $y^{(k)}$는 결국 원래의 Activation인 $x^{(k)}$가 복원된 결과가 될 것이다.\n",
    "\n",
    "$$ \\gamma^{(k)} = \\sqrt{Var[x^{(k)}]}, \\quad \\beta^{(k)} = \\mathbb{E}[x^{(k)}]$$\n",
    "\n",
    "또한 Batch Normalization의 또 다른 특징은 바로 미니배치 단위에서 정규화가 수행된다는 점이다. 모든 트레이닝 셋을 다 정규화에 활용할 수 있으면 좋겠지만 효율성을 위해서 미니배치를 Stochastic하게 샘플링하여 정규화를 수행하게 된다. 각각의 미니배치는 각각의 Activation에 대하여 Mean 및 Variance를 추정하는데 사용된다. 이러한 방식을 통해서 정규화에 사용되는 Statistics가 Backpropagation에 활용될 수 있게 된다.\n",
    "\n",
    "여기서 눈여겨 봐야할 점은 미니배치 정규화는 각 차원들의 Activation들 각각에 대해서 수행되는 Per-dimension Variance를 계산하게 된다는 점이다. 즉, 각 차원들의 Activation들을 독립적이라고 가정하고 각각의 Activation들 사이의 Joint Covariance를 고려하지 않는다는 의미이다. 만약 Joint Covariance를 계산하게 되는 경우에는 미니배치 사이즈가 Activation들의 개수보다 작다는 일반적인 사실에 의하여 Singular Covariance Matrix가 생성되는 결과를 가져올 수 있다.\n",
    "\n",
    "지금까지 설명한 Batch Normalization의 특징들을 정리하면 다음과 같다.\n",
    "\n",
    "* 트레이닝을 위한 미니배치 단위에서 수행\n",
    "* 미니배치 내의 한 Example 내에서의 Activation들은 각각 독립적이라고 가정\n",
    "* 독립적인 각각의 Activation들은 정규화를 위하여 미니배치 내의 Example들의 Statistics를 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization 레이어의 트레이닝\n",
    "\n",
    "Batch Normalization을 다음과 같이 정의하자. 미니배치 사이즈 $m$에 대한 미니배치 $B$를 $B = {x_1, ..., x_m}$과 같이 정의한다면 아래의 표현을 Batch Normalization 레이어의 연산으로 정의한다.\n",
    "\n",
    "$$ BN_{\\gamma, \\beta}: x_1, ..., x_m \\to y_1, ..., y_m $$\n",
    "\n",
    "Batch Normalization 레이어의 학습 과정은 다음과 같다.\n",
    "\n",
    "* 입력: 미니배치 $B$, 학습될 파라미터 $\\gamma$, $\\beta$ \n",
    "* 출력: ${y_i = BN_{\\gamma, \\beta}(x_i)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m}\\sum_{i=1}^m x_i \\\\\n",
    "\\sigma_{\\mathcal{B}}^2 & \\leftarrow \\frac{1}{m}\\sum_{i=1}^m(x_i - \\mu_{\\mathcal{B}})^2 \\\\\n",
    "\\widehat{x}_i & \\leftarrow \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}} \\\\\n",
    "y_i & \\leftarrow \\gamma \\widehat{x}_i + \\beta \\equiv \\text{BN}_{\\gamma, \\beta}(x_i)\n",
    "\\end{align*} \n",
    "$$ \n",
    " \n",
    " \n",
    " \n",
    "학습 과정에 대해서 좀 더 간단하게 정리하면 먼저 각 Activation의 Mean과 Variance를 미니배치 내에서 추정을 하여 Activation들을 각각 정규화를 시킨 다음에 파라미터 $\\gamma, \\beta$로 Scale 및 Shift를 수행하여 출력값을 내보내게 된다. 이 때 $\\gamma, \\beta$는 Backpropagation을 통해서 학습이 된다. 논문에 정리된 Backpropagation은 다음과 같다.\n",
    "\n",
    " \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\widehat{x}_i} & = \\frac{\\partial l}{\\partial y_i} \\cdot \\gamma \\\\\n",
    "\\frac{\\partial l}{\\partial \\sigma_{\\mathcal{B}}^2} & = \\sum_{i=1}^m \\frac{\\partial l}{\\partial \\widehat{x}_i} \\cdot (x_i - \\mu_{\\mathcal{B}}) \\cdot \\frac{-1}{2} \\cdot \\left( \\sigma_{\\mathcal{B}}^2 + \\epsilon \\right)^{-3/2} \\\\\n",
    "\\frac{\\partial l}{\\partial \\mu_{\\mathcal{B}}} & = \\left( \\sum_{i=1}^m \\frac{\\partial l}{\\partial \\widehat{x}_i} \\cdot \\frac{-1}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}} \\right) + \\frac{\\partial l}{\\partial \\sigma_{\\mathcal{B}}^2} \\cdot \\frac{\\sum_{i=1}^m -2(x_i - \\mu_{\\mathcal{B}})}{m} \\\\\n",
    "\\frac{\\partial l}{\\partial x_i} & = \\frac{\\partial l}{\\partial \\widehat{x}_i} \\cdot \\frac{1}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}} + \\frac{\\partial l}{\\partial \\sigma_{\\mathcal{B}}^2} \\cdot \\frac{2(x_i - \\mu_{\\mathcal{B}})}{m} + \\frac{\\partial l}{\\partial \\mu_{\\mathcal{B}}} \\cdot \\frac{1}{m} \\\\\n",
    "\\frac{\\partial l}{\\partial \\gamma} & = \\sum_{i=1}^m \\frac{\\partial l}{\\partial y_i} \\cdot \\widehat{x}_i \\\\\n",
    "\\frac{\\partial l}{\\partial \\beta} & = \\sum_{i=1}^m \\frac{\\partial l}{\\partial y_i}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization 레이어의 인퍼런스\n",
    "\n",
    "인퍼런스는 트레이닝과 과정이 조금 다르다. 트레이닝에서는 Activation의 정규화 과정에서 미니배치와의 Dependency를 고려하지만 인퍼런스에서도 이렇게 된다면 미니배치의 세팅에 따라서 결과가 달라지게 된다. 따라서 인퍼런스에서는 결과를 Deterministic하게 하기 위하여 고정된 Mean과 Variance를 이용하여 정규화를 한다.\n",
    "\n",
    "따라서 인퍼런스 전, 즉 트레이닝 과정에서 미리 미니배치를 뽑을 때 Sample Mean $\\mu_B$ 및 Sample Variance $\\sigma^2_B$를 이용하여 각각의 Moving Average $\\mathbb{E}_B[\\mu_B]$, $\\mathbb{E}_B[\\sigma^2_B]$를 구해놨어야 한다.\n",
    "\n",
    "Moving Average를 이용하여 실제 Mean 및 Variance를 추정하면 다음과 같다.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[x] & \\leftarrow \\mathbb{E}_{\\mathcal{B}}[\\mu_{\\mathcal{B}}] \\\\\n",
    "\\text{Var}[x] & \\leftarrow \\frac{m}{m-1} \\cdot \\mathbb{E}_{\\mathcal{B}}[\\sigma_{\\mathcal{B}}^2]\n",
    "\\end{align*}\n",
    "$$ \n",
    " \n",
    "이것을 이용하여 다음과 같이 인퍼런스를 수행한다.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\widehat{x}_i & \\leftarrow \\frac{x_i - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\\\\n",
    "y_i & \\leftarrow \\gamma \\widehat{x}_i + \\beta \\equiv \\text{BN}_{\\gamma, \\beta}(x_i)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hcnoh.github.io/2018-11-27-batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
