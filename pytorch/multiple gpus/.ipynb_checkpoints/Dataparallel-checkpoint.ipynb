{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch provides multi-gpu settings referred as 'Data Parallel' and this works like the below figures:\n",
    "\n",
    "<img src='./imgs/forward1.png'>\n",
    "<img src='./imgs/forward2.png'>\n",
    "<img src='./imgs/backward1.png'>\n",
    "<img src='./imgs/backward2.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë”¥ëŸ¬ë‹ì„ ì—¬ëŸ¬ ê°œì˜ GPUì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ ì¼ë‹¨ ëª¨ë¸ì„ ê° GPUì— ë³µì‚¬í•´ì„œ í• ë‹¹í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  iterationì„ í•  ë•Œë§ˆë‹¤ batchë¥¼ GPUì˜ ê°œìˆ˜ë§Œí¼ ë‚˜ëˆ•ë‹ˆë‹¤. ì´ë ‡ê²Œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì„ `scatter` í•œë‹¤ê³  í•˜ë©° ì‹¤ì œë¡œ Data Parallelì—ì„œ `scatter` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì…ë ¥ì„ ë‚˜ëˆ„ê³  ë‚˜ë©´ ê° GPUì—ì„œ forward ê³¼ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤. ê° ì…ë ¥ì— ëŒ€í•´ ëª¨ë¸ì´ ì¶œë ¥ì„ ë‚´ë³´ë‚´ë©´ ì´ì œ ì´ ì¶œë ¥ë“¤ì„ í•˜ë‚˜ì˜ GPUë¡œ ëª¨ìë‹ˆë‹¤. ì´ë ‡ê²Œ tensorë¥¼ í•˜ë‚˜ì˜ deviceë¡œ ëª¨ìœ¼ëŠ” ê²ƒì€ `gather` ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë³´í†µ ë”¥ëŸ¬ë‹ì—ì„œëŠ” ëª¨ë¸ì˜ ì¶œë ¥ê³¼ ì •ë‹µì„ ë¹„êµí•˜ëŠ” loss functionì´ ìˆìŠµë‹ˆë‹¤. Loss functionì„ í†µí•´ lossë¥¼ ê³„ì‚°í•˜ë©´ back-propagationì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Back-propagationì€ ê° GPUì—ì„œ ìˆ˜í–‰í•˜ë©° ê·¸ ê²°ê³¼ë¡œ ê° GPUì— ìˆë˜ ëª¨ë¸ì˜ gradientë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ 4ê°œì˜ GPUë¥¼ ì‚¬ìš©í•œë‹¤ë©´ 4ê°œì˜ GPUì— ê°ê° ëª¨ë¸ì´ ìˆê³  ê° ëª¨ë¸ì€ ê³„ì‚°ëœ gradientë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ê° GPUì— ìˆëŠ” gradientë¥¼ ë˜ í•˜ë‚˜ì˜ GPUë¡œ ëª¨ì•„ì„œ ì—…ë°ì´íŠ¸ë¥¼ í•©ë‹ˆë‹¤. ë§Œì•½ Adamê³¼ ê°™ì€ optimizerë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´ gradientë¡œ ë°”ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³  ì¶”ê°€ ì—°ì‚°ì„ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ Data Parallel ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ì´ ì½”ë“œ í•œ ì¤„ë¡œ ê°„ë‹¨íˆ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = BERT(args)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.cuda()\n",
    "\n",
    "...\n",
    "\n",
    "for i, (inputs, labels) in enumerate(trainloader):\n",
    "    outputs = model(inputs)          \n",
    "    loss = criterion(outputs, labels)     \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()                        \n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.DataParallel`ë¡œ modelì„ ê°ì‹¸ë©´ í•™ìŠµì„ í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ ëŒ€ë¡œ `replicate` â†’ `scatter` â†’ `parallel_apply` â†’ `gather` ìˆœì„œëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. \n",
    "\n",
    "```python\n",
    "def data_parallel(module, input, device_ids, output_device):\n",
    "    replicas = nn.parallel.replicate(module, device_ids)\n",
    "    inputs = nn.parallel.scatter(input, device_ids)\n",
    "    replicas = replicas[:len(inputs)]\n",
    "    outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
    "    return nn.parallel.gather(outputs, output_device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To solve the imablance problem of memory in use \n",
    "\n",
    "<img src='./imgs/memory_imbalance.png'>\n",
    "\n",
    "multi-gpusë¥¼ í™œìš©í•œë‹¤ê³  í•˜ë”ë¼ë„, loss gradientì˜ ê³„ì‚°ì€ í•˜ë‚˜ì˜ gpuì— ê²°ê³¼ê°’ë“¤ì„ ëª¨ì€ í›„ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— í•´ë‹¹í•˜ëŠ” gpuì˜ ë©”ëª¨ë¦¬ì˜ ì‚¬ìš©ëŸ‰ì€ í´ ìˆ˜ë°–ì— ì—†ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë©”ëª¨ë¦¬ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì œì¼ ê°„ë‹¨íˆ í•´ê²°í•˜ëŠ” ë°©ë²•ì€ ë‹¨ìˆœíˆ ì¶œë ¥ì„ ë‹¤ë¥¸ GPUë¡œ ëª¨ìœ¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ë””í´íŠ¸ë¡œ ì„¤ì •ë˜ì–´ìˆëŠ” GPUì˜ ê²½ìš° gradient ë˜í•œ í•´ë‹¹ GPUë¡œ ëª¨ì´ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ GPUì— ë¹„í•´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ìƒë‹¹íˆ ë§ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì¶œë ¥ì„ ë‹¤ë¥¸ GPUë¡œ ëª¨ìœ¼ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì˜ ì°¨ì´ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œì™€ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ì¶œë ¥ì„ ëª¨ìœ¼ê³  ì‹¶ì€ GPU ë²ˆí˜¸ë¥¼ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'\n",
    "model = nn.DataParallel(model, output_device=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customìœ¼ë¡œ DataParallel ì‚¬ìš©í•˜ê¸° ğŸ¥•\n",
    "\n",
    "`DataParallel`ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ì„œ ë©”ëª¨ë¦¬ ë¶ˆê· í˜•ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì— ëŒ€í•œ íŒíŠ¸ëŠ” [PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)ì´ë¼ëŠ” íŒ¨í‚¤ì§€ì— ìˆìŠµë‹ˆë‹¤. í•˜ë‚˜ì˜ GPUì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ ì¶œë ¥ì„ í•˜ë‚˜ì˜ GPUë¡œ ëª¨ì€ ê²ƒ ë•Œë¬¸ì…ë‹ˆë‹¤. ì™œ í•˜ë‚˜ì˜ GPUë¡œ ëª¨ë¸ì˜ ì¶œë ¥ì„ ëª¨ì„ê¹Œìš”? ì™œëƒí•˜ë©´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì‚¬ìš©í•´ì„œ loss functionì„ ê³„ì‚°í•´ì•¼í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ëª¨ë¸ì€ DataParallelì„ í†µí•´ ë³‘ë ¬ë¡œ ì—°ì‚°í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆì§€ë§Œ loss functionì´ ê·¸ëŒ€ë¡œì´ê¸° ë•Œë¬¸ì— í•˜ë‚˜ì˜ GPUì—ì„œ lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ loss function ë˜í•œ ë³‘ë ¬ë¡œ ì—°ì‚°í•˜ë„ë¡ ë§Œë“ ë‹¤ë©´ ë©”ëª¨ë¦¬ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì–´ëŠì •ë„ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch-Encoding ì¤‘ì—ì„œë„ ë‹¤ìŒ íŒŒì´ì¬ ì½”ë“œì— loss functionì„ parallelí•˜ê²Œ ë§Œë“œëŠ” ì½”ë“œê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functionì„ ë³‘ë ¬ ì—°ì‚° ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ë°©ë²•ì€ ëª¨ë¸ì„ ë³‘ë ¬ ì—°ì‚°ìœ¼ë¡œ ë§Œë“œëŠ” ë°©ë²•ê³¼ ë™ì¼í•©ë‹ˆë‹¤. PyTorchì—ì„œëŠ” loss function ë˜í•œ í•˜ë‚˜ì˜ ëª¨ë“ˆì…ë‹ˆë‹¤. ì´ ëª¨ë“ˆì„ ê° GPUì— replicate í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë°ì´í„°ì˜ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” tensorë¥¼ ê° GPUë¡œ scatter í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ëª¨ë¸ì˜ ì¶œë ¥, ì •ë‹µ, loss function ëª¨ë‘ ê° GPUì—ì„œ ì—°ì‚°í•  ìˆ˜ ìˆë„ë¡ ë°”ë€ ìƒíƒœì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê° GPUì—ì„œ loss ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° GPUì—ì„œëŠ” ê³„ì‚°í•œ lossë¡œ ë°”ë¡œ backward ì—°ì‚°ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<img src='./imgs/loss.png' width='1200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functionì„ parallel í•˜ê²Œ ë§Œë“¤ì–´ì„œ ì—°ì‚°í•˜ëŠ” ê³¼ì •ì„ ì½”ë“œë¡œ ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” targetì„ scatter í•œ ë‹¤ìŒì— replicateí•œ moduleì—ì„œ ê°ê° ê³„ì‚°ì„ í•©ë‹ˆë‹¤. ê³„ì‚°í•œ outputì™€ Reduce.applyë¥¼ í†µí•´ ê° GPUì—ì„œ backward ì—°ì‚°ì„ í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "\n",
    "class DataParallelCriterion(DataParallel):\n",
    "    def forward(self, inputs, *targets, **kwargs):\n",
    "        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)\n",
    "        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n",
    "        targets = tuple(targets_per_gpu[0] for targets_per_gpu in targets)\n",
    "        outputs = _criterion_parallel_apply(replicas, inputs, targets, kwargs)\n",
    "        return Reduce.apply(*outputs) / len(outputs), targets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataParallelCriterion`ì„ ì‚¬ìš©í•  ê²½ìš°ì— ì¼ë°˜ì ì¸ `DataParallel`ë¡œ ëª¨ë¸ì„ ê°ì‹¸ë©´ ì•ˆë©ë‹ˆë‹¤. `DataParallel`ì€ ê¸°ë³¸ì ìœ¼ë¡œ í•˜ë‚˜ì˜ GPUë¡œ ì¶œë ¥ì„ ëª¨ìœ¼ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ Custom DataParallel í´ë˜ìŠ¤ì¸ `DataParallelModel`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. `DataParallelModel`ê³¼ `DataParallelCriterion`ì„ ì‚¬ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì‚¬ìš©í•˜ëŠ” ë²•ì€ ìƒë‹¹íˆ ê°„ë‹¨í•©ë‹ˆë‹¤. Pytorch-Encoding íŒ¨í‚¤ì§€ì—ì„œ parallel.py íŒŒì¼ë§Œ ê°€ì ¸ì™€ì„œ í•™ìŠµ ì½”ë“œì—ì„œ import í•˜ë„ë¡ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "\n",
    "model = BERT(args)\n",
    "model = DataParallelModel(model)\n",
    "model.cuda()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "criterion = DataParallelCriterion(criterion) \n",
    "\n",
    "...\n",
    "\n",
    "for i, (inputs, labels) in enumerate(trainloader):\n",
    "    outputs = model(inputs)          \n",
    "    loss = criterion(outputs, labels)     \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()                        \n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë ‡ê²Œ í•™ìŠµì„ í•  ê²½ìš°ì— Nvidia-smi ì¶œë ¥ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. batch size ëŠ” 200ìœ¼ë¡œ ë™ì¼í•©ë‹ˆë‹¤. DataParallel ë§Œ ì‚¬ìš©í•  ë•Œì— ë¹„í•´ 1ë²ˆ GPUì™€ 2ë²ˆ GPUì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì˜ ì°¨ì´ê°€ ìƒë‹¹íˆ ì¤„ì—ˆìŠµë‹ˆë‹¤. batch sizeë¥¼ ê¸°ì¡´ì— ë¹„í•´ ëŠ˜ë¦´ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í•™ìŠµ ì‹œê°„ë„ ì „ì²´ì ìœ¼ë¡œ 1/3 ì •ë„ê°€ ì¤„ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ GPU-Utilì˜ ìˆ˜ì¹˜ë¡œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´ GPU ì„±ëŠ¥ì„ ì—¬ì „íˆ ì œëŒ€ë¡œ í™œìš© ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤. GPU ì„±ëŠ¥ì„ 100 %ë¡œ ëŒì–´ ì˜¬ë¦¬ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?\n",
    "\n",
    "<img src='./imgs/memory_balance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchì—ì„œ Distributed íŒ¨í‚¤ì§€ ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "ë¶„ì‚° í•™ìŠµ ìì²´ëŠ” í•˜ë‚˜ì˜ ì»´í“¨í„°ë¡œ í•™ìŠµí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ì»´í“¨í„°ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ê°œë°œëœ ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ multi-GPU í•™ìŠµì„ í•  ë•Œë„ ë¶„ì‚° í•™ìŠµì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶„ì‚° í•™ìŠµì„ ì§ì ‘ êµ¬í˜„í•  ìˆ˜ë„ ìˆì§€ë§Œ PyTorchì—ì„œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "PyTorchì—ì„œëŠ” DataParallelê³¼ í•¨ê»˜ ë¶„ì‚° í•™ìŠµê³¼ ê´€ë ¨ëœ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. PyTorchì—ì„œ ë¶„ì‚° í•™ìŠµì„ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´ ë‹¤ìŒ PyTorch Tutorialì„ ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/dist_tuto.html\n",
    "\n",
    "ë‹¨ìˆœíˆ ë¶„ì‚° í•™ìŠµì„ ì‚¬ìš©í•´ì„œ multi-GPU í•™ìŠµì„ í•˜ê³  ì‹¶ë‹¤ë©´ PyTorchì—ì„œ ê³µì‹ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” exampleì„ ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë¹„ì „ ë¶„ì•¼ì—ì„œ í° ë°ì´í„°ì…‹ ì¤‘ì— ìœ ëª…í•œ ê²ƒì´ ImageNet ì…ë‹ˆë‹¤. ë‹¤ìŒ ë§í¬ê°€ ImageNetì— ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œ ì˜ˆì œì…ë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œ ì—¬ëŸ¬ ë¨¸ì‹ ì—ì„œ ë¶„ì‚° í•™ìŠµì„ í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•˜ëŠ”ë° í•˜ë‚˜ì˜ ë¨¸ì‹ ì—ì„œ ì—¬ëŸ¬ GPU í•™ìŠµí•˜ëŠ” ë°©ë²•ë„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
    "\n",
    "https://github.com/pytorch/examples/blob/master/imagenet/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageNet ì˜ˆì œì˜ main.py ì—ì„œ multi-GPUì™€ ê´€ë ¨ëœ ì£¼ìš” ë¶€ë¶„ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•´ ë´¤ìŠµë‹ˆë‹¤. main.pyë¥¼ ì‹¤í–‰í•˜ë©´ mainì´ ì‹¤í–‰ë˜ëŠ”ë° mainì€ ë‹¤ì‹œ main_worker ë“¤ì„ multi-processingìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. GPU 4ê°œë¥¼ í•˜ë‚˜ì˜ ë…¸ë“œë¡œ ë³´ê³  world_sizeë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ mp.spawn í•¨ìˆ˜ê°€ 4ê°œì˜ GPUì—ì„œ ë”°ë¡œ ë”°ë¡œ main_workerë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    args.world_size = ngpus_per_node * args.world_size\n",
    "    mp.spawn(main_worker, nprocs=ngpus_per_node, \n",
    "             args=(ngpus_per_node, args))\n",
    "    \n",
    "    \n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    global best_acc1\n",
    "    args.gpu = gpu\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    \n",
    "    print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "    args.rank = args.rank * ngpus_per_node + gpu\n",
    "    dist.init_process_group(backend='nccl', \n",
    "                            init_method='tcp://127.0.0.1:FREEPORT',\n",
    "                            world_size=args.world_size, \n",
    "                            rank=args.rank)\n",
    "    \n",
    "    model = Bert()\n",
    "    model.cuda(args.gpu)\n",
    "    model = DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(args.num_epochs):\n",
    "        model = train(model)\n",
    "        acc = test(model, acc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main_workerì—ì„œ dist.init_process_groupì„ í†µí•´ ê° GPU ë§ˆë‹¤ ë¶„ì‚° í•™ìŠµì„ ìœ„í•œ ì´ˆê¸°í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. PyTorchì˜ docsë¥¼ ë³´ë©´ multi-GPU í•™ìŠµì„ í•  ê²½ìš° backendë¡œ ncclì„ ì‚¬ìš©í•˜ë¼ê³  ë‚˜ì™€ìˆìŠµë‹ˆë‹¤. init_methodì—ì„œ FREEPORTì— ì‚¬ìš© ê°€ëŠ¥í•œ portë¥¼ ì ìœ¼ë©´ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë¶„ì‚° í•™ìŠµì„ ìœ„í•œ ì´ˆê¸°í™”ë¥¼ í•˜ê³  ë‚˜ë©´ ë¶„ì‚° í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. 28ë²ˆì§¸ ì¤„ì„ ë³´ë©´ modelì—ëŠ” DataParallel ëŒ€ì‹ ì— DistributedDataParallelì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. DataParallelì—ì„œ ì–¸ê¸‰í•œ ì…ë ¥ì„ ë¶„ì‚°í•˜ê³  forward ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ë‹¤ì‹œ backward ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "https://pytorch.org/docs/stable/distributed.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderê°€ ì…ë ¥ì„ ê° í”„ë¡œì„¸ìŠ¤ì— ì „ë‹¬í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒì²˜ëŸ¼ DistributedSamplerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. DistributedSamplerëŠ” DistributedDataParallelê³¼ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ìš© ë°©ë²•ì€ ê°„ë‹¨í•˜ê²Œ ì •ì˜í•´ë†“ì€ datasetë¥¼ DistributedSamplerë¡œ ê°ì‹¸ì£¼ê³  DataLoaderì—ì„œ samplerì— ì¸ìë¡œ ë„£ì–´ì¤ë‹ˆë‹¤. ê·¸ ë‹¤ìŒì—” í‰ì†Œì— DataLoaderë¥¼ ì‚¬ìš©í•˜ë“¯ì´ ë˜‘ê°™ì´ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, ...)\n",
    "train_sampler = DistributedSampler(train_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistributedSamplerì˜ ë‚´ë¶€ë¥¼ ì‚´ì§ ë³´ìë©´ ë‹¤ìŒ ì½”ë“œì™€ ê°™ìŠµë‹ˆë‹¤(ë§ì€ ë¶€ë¶„ì„ ìƒëµí–ˆìŠµë‹ˆë‹¤). ê° SamplerëŠ” ì „ì²´ ë°ì´í„°ë¥¼ GPUì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆˆ ë¶€ë¶„ ë°ì´í„°ì—ì„œë§Œ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. ë¶€ë¶„ ë°ì´í„°ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì „ì²´ ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì€ ë‹¤ìŒì— ê·¸ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìª¼ê°œì„œ ê° GPU Samplerì— í• ë‹¹í•©ë‹ˆë‹¤. epoch ë§ˆë‹¤ ê° GPU samplerì— í• ë‹¹ë˜ëŠ” ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ëŠ” ë‹¤ì‹œ ë¬´ì‘ìœ„ë¡œ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” train_sampler.set_epoch(epoch) ëª…ë ¹ì–´ë¥¼ ë§¤ epoch ë§ˆë‹¤ í•™ìŠµ ì „ì— ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "class DistributedSampler(Sampler):\n",
    "    def __init__(self, dataset, num_replicas=None, rank=None):\n",
    "        num_replicas = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        self.dataset = dataset\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n",
    "        self.total_size = self.num_samples * self.num_replicas\n",
    "        \n",
    "    def __iter__(self):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.epoch)\n",
    "        indices = torch.randperm(len(self.dataset), generator=g).tolist()\n",
    "        indices = indices[self.rank:self.total_size:self.num_replicas]\n",
    "        return iter(indices)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Distributed íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•´ì„œ BERT ì‘ì€ ëª¨ë¸ì„ í•™ìŠµí•´ë´¤ìŠµë‹ˆë‹¤. Nvidia-smi ë¥¼ í†µí•´ í™•ì¸í•œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš© í˜„í™©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì™„ì „ ë™ì¼í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ GPU-Utilì˜ ìˆ˜ì¹˜ë„ 99%ë¡œ ìƒë‹¹íˆ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ê¹Œì§€ ì™”ë‹¤ë©´ multi-GPU í•™ìŠµì„ ì œëŒ€ë¡œ í•  ì¤€ë¹„ê°€ ëìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ Distibuted DataParallelì˜ ê²½ìš° í•™ìŠµì„ ì‹œì‘í•˜ë ¤ í•  ë•Œ ê°„ê°„íˆ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ github issue ê¸€ì´ ì—¬ëŸ¬ ë¬¸ì œ ì¤‘ì— í•˜ë‚˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. BERT ì½”ë“œë¥¼ ëŒë¦´ ë•Œë„ ì—ëŸ¬ê°€ ë°œìƒí–ˆëŠ”ë° ëª¨ë¸ì—ì„œ í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” parameterê°€ ìˆì„ ê²½ìš°ì— Distributed DataParallelì´ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì˜ê²¬ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì‹ ê²½ì“°ì§€ ì•Šê³  í•™ìŠµì„ í•˜ê¸° ìœ„í•´ì„œ ì°¾ì•„ë³´ë‹¤ê°€ Nvidiaì—ì„œ ë§Œë“  Apexë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "https://github.com/facebookresearch/maskrcnn-benchmark/issues/318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nvidia Apexë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•˜ê¸° ğŸ¥•\n",
    "\n",
    "Nvidiaì—ì„œ Apexë¼ëŠ” Mixed Precision ì—°ì‚°ì„ ìœ„í•œ íŒ¨í‚¤ì§€ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ë³´í†µ ë”¥ëŸ¬ë‹ì€ 32 ë¹„íŠ¸ ì—°ì‚°ì„ í•˜ëŠ”ë° 16 ë¹„íŠ¸ ì—°ì‚°ì„ ì‚¬ìš©í•´ì„œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê² ë‹¤ëŠ” ì˜ë„ë¡œ ë§Œë“  ê²ƒì…ë‹ˆë‹¤. Apexì—ëŠ” Mixed Precision ì—°ì‚° ê¸°ëŠ¥ ë§ê³ ë„ Distributed ê´€ë ¨ ê¸°ëŠ¥ì´ í¬í•¨í•©ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Mixed Precisionì— ëŒ€í•œ ë‚´ìš©ì€ ë‹¤ë£¨ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "Apexì˜ Distributed DataParallel ê¸°ëŠ¥ì„ í•˜ëŠ” ê²ƒì´ DDP ì…ë‹ˆë‹¤. Apexì—ì„œ ImageNet í•™ìŠµì„ ìœ„í•´ ë§Œë“  ì˜ˆì œì— ê´€ë ¨ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤. Apex ì‚¬ìš©ë²•ì€ Docsì— ì˜ ë‚˜ì™€ìˆìœ¼ë‹ˆ ì‚´í´ë³´ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ ì½”ë“œ 2ë²ˆ ì¤„ì—ì„œ ë³´ë“¯ì´ apexì—ì„œ DistributedDataParallelì„ import í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤. ìœ„ PyTorch ê³µì‹ ì˜ˆì œì—ì„œì™€ëŠ” ë‹¬ë¦¬ ì½”ë“œ ë‚´ì—ì„œ ë©€í‹° í”„ë¡œì„¸ì‹±ì„ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 19 ì¤„ì—ì„œ ë³´ë“¯ì´ DDPë¡œ modelì„ ê°ì‹¸ì¤ë‹ˆë‹¤. ê·¸ ì´ì™¸ì—ëŠ” PyTorch DistributedDataParallelê³¼ ë™ì¼í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "import torch.distributed as dist\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args\n",
    "    \n",
    "    args.gpu = 0\n",
    "    args.world_size = 1\n",
    "    \n",
    "    args.gpu = args.local_rank\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    torch.distributed.init_process_group(backend='nccl',\n",
    "                                         init_method='env://')\n",
    "    args.world_size = torch.distributed.get_world_size()\n",
    "    \n",
    "    model = Bert()\n",
    "    model.cuda(args.gpu)\n",
    "    model = DDP(model, delay_allreduce=True)\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(args.num_epochs):\n",
    "        model = train(model)\n",
    "        acc = test(model, acc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•  ë•ŒëŠ” ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤. Torch.distributed.launchë¥¼ í†µí•´ main.pyë¥¼ ì‹¤í–‰í•˜ëŠ”ë° ë…¸ë“œì—ì„œ 4ê°œì˜ í”„ë¡œì„¸ìŠ¤ê°€ ëŒì•„ê°€ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. ê° í”„ë¡œì„¸ìŠ¤ëŠ” GPU í•˜ë‚˜ì—ì„œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ë§Œì•½ GPUê°€ 2ê°œë¼ë©´ nproc_per_nodeë¥¼ 2ë¡œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤. main.pyì— batch_sizeì™€ num_workerë¥¼ ì„¤ì •í•˜ëŠ”ë° ê° GPU ë§ˆë‹¤ì˜ batch_sizeì™€ worker ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. batch sizeê°€ 60ì´ê³  workerì˜ ìˆ˜ê°€ 2ë¼ë©´ ì „ì²´ì ìœ¼ë¡œëŠ” batch sizeê°€ 240ì´ë©° workerì˜ ìˆ˜ëŠ” 8ì…ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "python -m torch.distributed.launch --nproc_per_node=4 main.py \\\n",
    "    --batch_size 60 \\\n",
    "    --num_workers 2 \\\n",
    "    --gpu_devices 0 1 2 3\\\n",
    "    --distributed \\\n",
    "    --log_freq 100 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia Apexë¥¼ ì‚¬ìš©í•´ì„œ multi-GPU í•™ìŠµì„ í–ˆìŠµë‹ˆë‹¤. GPU ì‚¬ìš© í˜„í™©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ëª¨ë“  GPUì—ì„œ ì¼ì •í•©ë‹ˆë‹¤.(3ë²ˆ GPUëŠ” ë‹¤ë¥¸ ì‘ì—…ì´ í• ë‹¹ë°›ê³  ìˆê¸° ë•Œë¬¸ì— ì¡í˜€ìˆìŠµë‹ˆë‹¤). GPU-Utilì„ ë³´ë©´ 99% ì•„ë‹ˆë©´ 100 %ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU í•™ìŠµ ë°©ë²• ì„ íƒí•˜ê¸° ğŸ¥•\n",
    "ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ PyTorchë¡œ multi-GPU í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ 3ê°€ì§€ ì…ë‹ˆë‹¤.\n",
    "DataParallel\n",
    "Custom DataParallel\n",
    "Distributed DataParallel\n",
    "Nvidia Apex\n",
    "DataParallelì€ PyTorchì—ì„œ ì œê³µí•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì´ì§€ë§Œ GPU ë©”ëª¨ë¦¬ ë¶ˆê· í˜• ë¬¸ì œê°€ ìƒê²¼ìŠµë‹ˆë‹¤. Custom DataParallelì˜ ê²½ìš° GPU ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ ì–´ëŠì •ë„ í•´ê²°í•´ì£¼ì§€ë§Œ GPUë¥¼ ì œëŒ€ë¡œ í™œìš©í•˜ì§€ ëª»í•œë‹¤ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. Distributed DataParallelì€ ì›ë˜ ë¶„ì‚°í•™ìŠµì„ ìœ„í•´ ë§Œë“¤ì–´ì§„ PyTorchì˜ ê¸°ëŠ¥ì´ì§€ë§Œ multi-GPU í•™ìŠµì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆê³  ë©”ëª¨ë¦¬ ë¶ˆê· í˜• ë¬¸ì œì™€ GPUë¥¼ í™œìš©í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œê°€ ì—†ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê°„ê°„íˆ ë¬¸ì œê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— Nvidiaì—ì„œ ë§Œë“  Apexë¥¼ ì´ìš©í•´ì„œ multi-GPU í•™ìŠµí•˜ëŠ” ê²ƒì„ ì‚´í´ë´¤ìŠµë‹ˆë‹¤.\n",
    "ê·¸ë ‡ë‹¤ë©´ Apexë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•­ìƒ ì¢‹ì„ê¹Œìš”? ì œê°€ ì‚´í´ë³¸ ì´ëŸ° ë¬¸ì œë“¤ì´ ë”¥ëŸ¬ë‹ í•™ìŠµì„ í•  ë•Œ í•­ìƒ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§Œì•½ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ í•™ìŠµí•œë‹¤ë©´ DataParallel ë§Œìœ¼ë¡œ ì¶©ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. BERTì—ì„œ GPU ë©”ëª¨ë¦¬ ë¶ˆê· í˜• ë¬¸ì œê°€ ìƒê¸°ëŠ” ì´ìœ ëŠ” ëª¨ë¸ ì¶œë ¥ì´ ìƒë‹¹íˆ í¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê° stepë§ˆë‹¤ wordì˜ ê°œìˆ˜ë§Œí¼ì´ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ì´ëŸ° ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë¯¸ì§€ ë¶„ë¥˜ì˜ ê²½ìš° ëª¨ë¸ ìì²´ê°€ í´ ìˆ˜ëŠ” ìˆì–´ë„ ëª¨ë¸ ì¶œë ¥ì€ ê·¸ë ‡ê²Œ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ GPU ë©”ëª¨ë¦¬ ë¶ˆê· í˜•ì€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ CIFAR-10ì— PyramidNetì„ í•™ìŠµí•´ë´¤ìŠµë‹ˆë‹¤. í•™ìŠµì— ì‚¬ìš©í•œ ì½”ë“œ ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. CIFAR-10ì€ 10ê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì§„ 32x32ì˜ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì…‹ ì…ë‹ˆë‹¤. ë˜í•œ PyramidNetì€ CIFAR-10 ì—ì„œ ìµœê·¼ê¹Œì§€ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ëƒˆë˜ ëª¨ë¸ì…ë‹ˆë‹¤. PyramidNetì€ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Multi-GPUì—ì„œ í•™ìŠµ ì„±ëŠ¥ì„ ë¹„êµí•˜ë ¤ë©´ ì‚¬ì´ì¦ˆê°€ í° ëª¨ë¸ì„ ì“°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ íŒŒë¼ë©”í„°ì˜ ê°œìˆ˜ê°€ 24,253,410ì¸ ëª¨ë¸ì„ ì‹¤í—˜ì— ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í‘œì—ì„œ PyramidNet(alpha=270)ì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. í•™ìŠµì—ëŠ” K80 4ê°œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/dataparallel-imbalanced-memory-usage/22551\n",
    "\n",
    "https://discuss.pytorch.org/t/multi-gpu-training-memory-usage-in-balance/4163/4\n",
    "\n",
    "https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\n",
    "\n",
    "https://github.com/dnddnjs/pytorch-multigpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
