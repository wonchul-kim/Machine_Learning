{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from build_vocab import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, backbone, pretrained, fine_tune):\n",
    "        super(Encoder, self).__init__()\n",
    "        if backbone == 'densenet201':\n",
    "            model = models.densenet201(pretrained=pretrained)\n",
    "            self.num_ftrs = model.classifier.in_features\n",
    "            self.features = model.features\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_ftrs, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.bn(self.fc(x))\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, length):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embed), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = Image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        img = transform(img).unsqueeze(0)\n",
    "        \n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)])\n",
    "    \n",
    "    with open(args.vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    encoder = Encoder(args.embed_size, 'densenet201', True, False).eval().to(device)\n",
    "    decoder = Decoder(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(args.encoder_path))\n",
    "    decoder.load_state_dict(torch.load(args.decoder_path))\n",
    "    \n",
    "    img = load_image(args.image, transform)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
