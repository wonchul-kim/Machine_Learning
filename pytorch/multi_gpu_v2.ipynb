{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/residentmario/full-batch-mini-batch-and-online-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full batch, mini-batch, and online-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important optimization to make early into the process of building a neural network is selecting an appropriate batch size.\n",
    "\n",
    "Neural networks are trained in a series of epochs. Each epoch consists of one forward pass and one backpropogation pass over all of the provided training samples. Naively, we can compute the true gradient by computing the gradient value of each training case independently, then summing together the resultant vectors. This is known as full batch learning, and it provides an exact answer to the question of which stepping direction is optimal, as far as gradient descent is concerned.\n",
    "\n",
    "Alternatively, we may choose to update the training weights several times over the course of a single epoch. ***In this case, we are no longer computing the true gradient; instead we are computing an approximation of the true gradient, using however many training samples are included in each split of the epoch. This is known as mini-batch learning.***\n",
    "\n",
    "> computing not true gradient??? why??\n",
    "\n",
    "In the most extreme case we may choose to adjust the gradient after every single forward and backwards pass. This is known as online learning.\n",
    "\n",
    "The amount of data included in each sub-epoch weight change is known as the batch size. For example, with a training dataset of 1000 samples, a full batch size would be 1000, a mini-batch size would be 500 or 200 or 100, and an online batch size would be just 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tradeoffs\n",
    "\n",
    "There are many tradeoffs implicit in the choice of batch size.\n",
    "\n",
    "Full batch learn is simpler to reason about, as we know that every step the learner makes is precisely aligned with the ***true gradient***. In this sense it is less random than mini-batch or online learning, both of which take steps that are dependent on ***the randomness of the batch selection process***. As a result, full batch learning will always want to take nice smooth steps towards the globally optimal decision point; an attractive property, to be sure. ***However, to be efficient full batch learning requires that the entire training dataset be retained in memory, and hence it hits the scaling ceiling very very quickly.***\n",
    "\n",
    "> true gradient?????\n",
    "\n",
    "> batch selection process? gradient가 가정 화실한 방향으로 가는거 아닌가?\n",
    "\n",
    "> retained in memory? hits the sacaling ceiling?\n",
    "\n",
    "\n",
    "Mini-batch learning is more exposed to randomness in the dataset and in the choice of the batch size, resulting in weight steps that look significantly more random than full batch steps. ***The smaller the batch size, the greater the randomness***. On the other hand, assuming an appropriate batch size is chosen, they train much more quickly than full batch learners. Full batch learners must perform the full dataset scan for every single weight update. Mini-batch learners get to perform that same weight update multiple times per dataset scan. Assuming you choose an representative batch size this results in multiplicatively faster training.\n",
    "\n",
    "***Online learners are the most random of all. Because the steps they take are all over the place, they're significantly harder to debug. For this reason they are not usually used in static applications. However they are useful for applications that perform machine learning a runtime, as they eliminate the need for an expensive batch recomputation on reaching arbitrary input volume thresholds.***\n",
    "\n",
    "> online은 단순히 매 epoch마다 gradient가 변화하는거 아닌가? 그런데 기존에 min-batch에서도 gradient는 매번 업데이트 되지 않ㄴ나???????\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practices\n",
    "\n",
    "In practice, most practitioners use mini-batch learning as their go-to. ***Full batch learning is generally reserved for very small datasets, and online learning is primarily the domain of sophisticated production data systems that live in production.*** The process of determining the correct batch size is left to experimentation, as it is highly data-dependent. The choice of batch size does not make a strong difference between different model architecture (or rather, it does not have nearly as strong an effect as many other models optimizations you should examine first). So to determine your batch size, it's reasonable to build a toy model, try a few different batch sizes, pick the one that seems to converge the fastest, and proceed from there. Even if you change the model, the batch size you chose earlier on will retain being a good choice.\n",
    "\n",
    "> online learning은 정확하게 머지? 매 epoch마다 gradient를 업데이트하겠다는 거 아닌가???\n",
    "\n",
    "\n",
    "Batch sizes that are multiples of powers of 2 are common. E.g. 32, 64, 128, and so on. I'm not sure whether this is just a stylistic point, or if there is actually any optimization going on around being a binary power.\n",
    "\n",
    "Batch computations are heavily vectorized in their implementation, so the difference in processing speed between a 32 batch and a 64 batch is less than double, as you would naively assume. Smaller mini-batches are less efficient than larger ones in gradient calculation terms, but can make up for it with faster model convergence speeds, which may necessitate fewer epochs total. Online learning is least efficient of all, as it features zero vectorization, and is exponentially slower than online learning (on the toy dataset we will see shortly, 1000 observations with 3 output classes and 100 epochs take less than a second to train full-batch, and one point five minutes to train online).\n",
    "\n",
    "On the other hand, there is evidence in practice that \"large\" batches tend to converge to minima with poorer generalization characteristics. This is because larger batches are more likely to converge to so-called \"sharp\" minima, e.g. sink values that are reasonably good, but do not provide the best problem solutions, but which have steep sides, and thus the learners are less able to escape from. Smaller batches are more likely to converge to \"flat\" minima. They are more able to escape these sinks, if need be. Reference here.\n",
    "\n",
    "Furthermore, when you put  m  more examples into a mini-batch, you reduce the uncertaincy in the gradient by a factor of only  O(m−−√)  or so (source).\n",
    "\n",
    "Another factor to consider when selecting a batch size is the learning rate. The interplay between the learning rate and the batch size is subtle, but important. A larger learning rate will compensate for a reliable slow-learning gradient, and a smaller learning rate will compensate for a more random fast-learning gradient. This visualization, taken from this blog post on the subject, shows a comparison between larger and smaller batch sizes, the learning rate, and the error rate the model ultimately converges to:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
