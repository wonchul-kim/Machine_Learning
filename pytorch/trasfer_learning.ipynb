{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "[Almost any Image Classification Problem using PyTorch](https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4)\n",
    "\n",
    "[Plant Seedlings Classification](https://www.kaggle.com/c/plant-seedlings-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytoch offers several pre-trained models and they can be easily loaded. Its main aim is to experiment faster using transfer learning on all available pre-trained models. \n",
    "\n",
    "\n",
    "**The following pre-trained models are available on PyTorch**\n",
    "\n",
    "* resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "* squeezenet1_0, squeezenet1_1\n",
    "\n",
    "* Alexnet\n",
    "\n",
    "* inception_v3\n",
    "\n",
    "* Densenet121, Densenet169, Densenet201\n",
    "\n",
    "* Vgg11, vgg13, vgg16, vgg19, vgg11_bn. vgg13_bn, vgg16_bn, vgg19_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The three cases in Transfer Learning \n",
    "\n",
    "1. Freezing all the layers except the final one\n",
    "\n",
    "2. Freezing the first few layers\n",
    "\n",
    "3. Fine-tuning the entire network.\n",
    "\n",
    "All the models used above are written differently. Some use Sequential containers, which contain many layers and some directly contain just the layer. So it is important to check how these models are defined in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet and Inception_V3\n",
    "\n",
    "Since the Imagenet dataset has 1000 layers, We need to change the last layer as per our requirement. ***We can freeze whichever layer we don’t want to train and pass the remaining layer parameters to the optimizer.***\n",
    "\n",
    "> 이미 만들어진 모델을 가져와서 쓰는데 그 모델 내부 layer에 대한 학습여부를 선택할 수 있나??\n",
    "\n",
    "```python\n",
    "if resnet:\n",
    "    model_conv=torchvision.models.resnet50()\n",
    "if inception:\n",
    "  model_conv=torchvision.models.inception_v3()\n",
    "## Change the last layer\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, n_class)\n",
    "```\n",
    "\n",
    "This model_conv has, In PyTorch there are children (containers) and each children has several childs (layers). Below is the example for resnet50,\n",
    "\n",
    "```python\n",
    "for name, child in model_conv.named_children():\n",
    "    for name2, params in child.named_parameters():\n",
    "        print(name, name2)\n",
    "```\n",
    "\n",
    "A long list of param are listed, some of them are shown below,\n",
    "conv1 weight\n",
    "```\n",
    "bn1 weight\n",
    "bn1 bias\n",
    "....\n",
    "fc weight\n",
    "fc bias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to freeze few layers before training, We can simple do using the following command:\n",
    "\n",
    "```python\n",
    "## Freezing all layers\n",
    "for params in model_conv.parameters():\n",
    "    params.requires_grad = False\n",
    "## Freezing the first few layers. Here I am freezing the first 7 layers \n",
    "ct = 0\n",
    "for name, child in model_conv.named_children():\n",
    "    ct += 1\n",
    "    if ct < 7:\n",
    "        for name2, params in child.named_parameters():\n",
    "        params.requires_grad = False\n",
    "```\n",
    "\n",
    "Changing the last layer to fit our new_data is a bit tricky and we need to carefully check how the underlying layers are represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    if model_name is 'resnet':\n",
    "        model_conv = torchvision.models.resnet50()\n",
    "    elif model_name is 'inception':\n",
    "        model_conv = torchvision.models.inception_v3()\n",
    "        \n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = get_model('resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 10\n",
    "nb_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(nb_ftrs, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 weight\n",
      "bn1 weight\n",
      "bn1 bias\n",
      "layer1 0.conv1.weight\n",
      "layer1 0.bn1.weight\n",
      "layer1 0.bn1.bias\n",
      "layer1 0.conv2.weight\n",
      "layer1 0.bn2.weight\n",
      "layer1 0.bn2.bias\n",
      "layer1 0.conv3.weight\n",
      "layer1 0.bn3.weight\n",
      "layer1 0.bn3.bias\n",
      "layer1 0.downsample.0.weight\n",
      "layer1 0.downsample.1.weight\n",
      "layer1 0.downsample.1.bias\n",
      "layer1 1.conv1.weight\n",
      "layer1 1.bn1.weight\n",
      "layer1 1.bn1.bias\n",
      "layer1 1.conv2.weight\n",
      "layer1 1.bn2.weight\n",
      "layer1 1.bn2.bias\n",
      "layer1 1.conv3.weight\n",
      "layer1 1.bn3.weight\n",
      "layer1 1.bn3.bias\n",
      "layer1 2.conv1.weight\n",
      "layer1 2.bn1.weight\n",
      "layer1 2.bn1.bias\n",
      "layer1 2.conv2.weight\n",
      "layer1 2.bn2.weight\n",
      "layer1 2.bn2.bias\n",
      "layer1 2.conv3.weight\n",
      "layer1 2.bn3.weight\n",
      "layer1 2.bn3.bias\n",
      "layer2 0.conv1.weight\n",
      "layer2 0.bn1.weight\n",
      "layer2 0.bn1.bias\n",
      "layer2 0.conv2.weight\n",
      "layer2 0.bn2.weight\n",
      "layer2 0.bn2.bias\n",
      "layer2 0.conv3.weight\n",
      "layer2 0.bn3.weight\n",
      "layer2 0.bn3.bias\n",
      "layer2 0.downsample.0.weight\n",
      "layer2 0.downsample.1.weight\n",
      "layer2 0.downsample.1.bias\n",
      "layer2 1.conv1.weight\n",
      "layer2 1.bn1.weight\n",
      "layer2 1.bn1.bias\n",
      "layer2 1.conv2.weight\n",
      "layer2 1.bn2.weight\n",
      "layer2 1.bn2.bias\n",
      "layer2 1.conv3.weight\n",
      "layer2 1.bn3.weight\n",
      "layer2 1.bn3.bias\n",
      "layer2 2.conv1.weight\n",
      "layer2 2.bn1.weight\n",
      "layer2 2.bn1.bias\n",
      "layer2 2.conv2.weight\n",
      "layer2 2.bn2.weight\n",
      "layer2 2.bn2.bias\n",
      "layer2 2.conv3.weight\n",
      "layer2 2.bn3.weight\n",
      "layer2 2.bn3.bias\n",
      "layer2 3.conv1.weight\n",
      "layer2 3.bn1.weight\n",
      "layer2 3.bn1.bias\n",
      "layer2 3.conv2.weight\n",
      "layer2 3.bn2.weight\n",
      "layer2 3.bn2.bias\n",
      "layer2 3.conv3.weight\n",
      "layer2 3.bn3.weight\n",
      "layer2 3.bn3.bias\n",
      "layer3 0.conv1.weight\n",
      "layer3 0.bn1.weight\n",
      "layer3 0.bn1.bias\n",
      "layer3 0.conv2.weight\n",
      "layer3 0.bn2.weight\n",
      "layer3 0.bn2.bias\n",
      "layer3 0.conv3.weight\n",
      "layer3 0.bn3.weight\n",
      "layer3 0.bn3.bias\n",
      "layer3 0.downsample.0.weight\n",
      "layer3 0.downsample.1.weight\n",
      "layer3 0.downsample.1.bias\n",
      "layer3 1.conv1.weight\n",
      "layer3 1.bn1.weight\n",
      "layer3 1.bn1.bias\n",
      "layer3 1.conv2.weight\n",
      "layer3 1.bn2.weight\n",
      "layer3 1.bn2.bias\n",
      "layer3 1.conv3.weight\n",
      "layer3 1.bn3.weight\n",
      "layer3 1.bn3.bias\n",
      "layer3 2.conv1.weight\n",
      "layer3 2.bn1.weight\n",
      "layer3 2.bn1.bias\n",
      "layer3 2.conv2.weight\n",
      "layer3 2.bn2.weight\n",
      "layer3 2.bn2.bias\n",
      "layer3 2.conv3.weight\n",
      "layer3 2.bn3.weight\n",
      "layer3 2.bn3.bias\n",
      "layer3 3.conv1.weight\n",
      "layer3 3.bn1.weight\n",
      "layer3 3.bn1.bias\n",
      "layer3 3.conv2.weight\n",
      "layer3 3.bn2.weight\n",
      "layer3 3.bn2.bias\n",
      "layer3 3.conv3.weight\n",
      "layer3 3.bn3.weight\n",
      "layer3 3.bn3.bias\n",
      "layer3 4.conv1.weight\n",
      "layer3 4.bn1.weight\n",
      "layer3 4.bn1.bias\n",
      "layer3 4.conv2.weight\n",
      "layer3 4.bn2.weight\n",
      "layer3 4.bn2.bias\n",
      "layer3 4.conv3.weight\n",
      "layer3 4.bn3.weight\n",
      "layer3 4.bn3.bias\n",
      "layer3 5.conv1.weight\n",
      "layer3 5.bn1.weight\n",
      "layer3 5.bn1.bias\n",
      "layer3 5.conv2.weight\n",
      "layer3 5.bn2.weight\n",
      "layer3 5.bn2.bias\n",
      "layer3 5.conv3.weight\n",
      "layer3 5.bn3.weight\n",
      "layer3 5.bn3.bias\n",
      "layer4 0.conv1.weight\n",
      "layer4 0.bn1.weight\n",
      "layer4 0.bn1.bias\n",
      "layer4 0.conv2.weight\n",
      "layer4 0.bn2.weight\n",
      "layer4 0.bn2.bias\n",
      "layer4 0.conv3.weight\n",
      "layer4 0.bn3.weight\n",
      "layer4 0.bn3.bias\n",
      "layer4 0.downsample.0.weight\n",
      "layer4 0.downsample.1.weight\n",
      "layer4 0.downsample.1.bias\n",
      "layer4 1.conv1.weight\n",
      "layer4 1.bn1.weight\n",
      "layer4 1.bn1.bias\n",
      "layer4 1.conv2.weight\n",
      "layer4 1.bn2.weight\n",
      "layer4 1.bn2.bias\n",
      "layer4 1.conv3.weight\n",
      "layer4 1.bn3.weight\n",
      "layer4 1.bn3.bias\n",
      "layer4 2.conv1.weight\n",
      "layer4 2.bn1.weight\n",
      "layer4 2.bn1.bias\n",
      "layer4 2.conv2.weight\n",
      "layer4 2.bn2.weight\n",
      "layer4 2.bn2.bias\n",
      "layer4 2.conv3.weight\n",
      "layer4 2.bn3.weight\n",
      "layer4 2.bn3.bias\n",
      "fc weight\n",
      "fc bias\n"
     ]
    }
   ],
   "source": [
    "for name, child in model_conv.named_children():\n",
    "    for name2, params in child.named_parameters():\n",
    "        print(name, name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freezing all layers\n",
    "for name, child in model_conv.named_children():\n",
    "    for params in child.parameters():\n",
    "#         print(params)\n",
    "        params.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "## Freezing the first few layers. Here I am freezing the first 7 layers \n",
    "cnt = 0\n",
    "for name, child in model_conv.named_children():\n",
    "    cnt += 1\n",
    "    if cnt < 7:\n",
    "        for name2, params in child.named_parameters():\n",
    "            params.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squeeze-Net\n",
    "\n",
    "There are two variants of squeeze-net in PyTorch and Squeeze-net final layer is wrapped inside a container(Sequential). So we need to first list all the children layers inside it and convert the required layers according to our dataset and convert into back to a container and write it back to the class. \n",
    "\n",
    "```python\n",
    "model_conv = torchvision.models.squeezenet1_1()\n",
    "for name, params in model_conv.named_children():\n",
    "    print(name)\n",
    "```\n",
    "\n",
    "features<br/>\n",
    "classifier\n",
    "\n",
    "```python\n",
    "## How many In_channels are there for the conv layer\n",
    "in_ftrs = model_conv.classifier[1].in_channels\n",
    "## How many Out_channels are there for the conv layer\n",
    "out_ftrs = model_conv.classifier[1].out_channels\n",
    "## Converting a sequential layer to list of layers \n",
    "features = list(model_conv.classifier.children())\n",
    "## Changing the conv layer to required dimension\n",
    "features[1] = nn.Conv2d(in_ftrs, n_class, kernel_size,stride)\n",
    "## Changing the pooling layer as per the architecture output\n",
    "features[3] = nn.AvgPool2d(12, stride=1)\n",
    "## Making a container to list all the layers\n",
    "model_conv.classifier = nn.Sequential(*features)\n",
    "## Mentioning the number of out_put classes\n",
    "model_conv.num_classes = n_class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense-Net\n",
    "\n",
    "It is very similar to Resnet but the last layer is named as classifier. \n",
    "\n",
    "```python\n",
    "model_conv = torchvision.models.densenet121(pretrained='imagenet')\n",
    "num_ftrs = model_conv.classifier.in_features\n",
    "model_conv.classifier = nn.Linear(num_ftrs, n_class)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG and Alex-Net\n",
    "\n",
    "It is similar to Squeeze-net. The last fc layers are wrapped inside a container, so we need to read that container and change the last fc layer as per our dataset requirements.\n",
    "\n",
    "```python\n",
    "model_conv = torchvision.models.vgg19(pretrained='imagenet')\n",
    "# Number of filters in the bottleneck layer\n",
    "num_ftrs = model_conv.classifier[6].in_features\n",
    "# convert all the layers to list and remove the last one\n",
    "features = list(model_conv.classifier.children())[:-1]\n",
    "## Add the last layer based on the num of classes in our dataset\n",
    "features.extend([nn.Linear(num_ftrs, n_class)])\n",
    "## convert it into container and add it to our model class.\n",
    "model_conv.classifier = nn.Sequential(*features)\n",
    "```\n",
    "\n",
    "We have seen how to freeze required layers and change the last layer for different networks. Now lets train the network using one of the nets. I am not going to mention this here in detail as it is already made available in my Github repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
