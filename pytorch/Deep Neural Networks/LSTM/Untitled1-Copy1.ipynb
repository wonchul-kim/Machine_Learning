{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_classes = 5\n",
    "input_size = 5 # one-hot size \n",
    "hidden_size = 5 #output from the cell. 바로 결과 예측을 위해서 5로 설정 \n",
    "batch_size = 1 # sentence의 개수 (단어개수) \n",
    "sequence_length = 1 # 이번에는 한번에 하나씩 해본다 \n",
    "num_layers = 1 # one layer rnn이다. (아직 안 다룬 개념)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0],\n",
      " [0, 1, 0, 0, 0],\n",
      " [1, 0, 0, 0, 0],\n",
      " [0, 0, 1, 0, 0],\n",
      " [0, 0, 0, 1, 0],\n",
      " [0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "idx2char = ['h','i','e','l','o']\n",
    "x_data = [0,1,0,2,3,3] #hihell \n",
    "\n",
    "one_hot_lookup = [[1,0,0,0,0], #h \n",
    "                  [0,1,0,0,0], \n",
    "                  [0,0,1,0,0], \n",
    "                  [0,0,0,1,0], \n",
    "                  [0,0,0,0,1]] \n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] \n",
    "pp.pprint(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = [1,0,2,3,3,4] #ihello \n",
    "inputs = torch.Tensor(x_one_hot) \n",
    "labels = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Model,self).__init__() \n",
    "        self.rnn=nn.RNN(input_size = input_size, \\\n",
    "                        hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x, hidden): \n",
    "        #input x 를 (batch_size,sequence_length,input_size)로 reshape\n",
    "        x = x.view(batch_size, sequence_length, input_size) \n",
    "        \n",
    "        #Propagate input through RNN \n",
    "        #Input:(batch,seq_len,input_size) \n",
    "        out,hidden = self.rnn(x, hidden) \n",
    "        \n",
    "        #for make sure, output이 N * 5 shape을 따르게 하기 위해서 \n",
    "        #out = out.view(-1, num_classes)\n",
    "        \n",
    "        return hidden, out \n",
    "    \n",
    "    def init_hidden(self): \n",
    "        #initialize hidden and cell states \n",
    "        \n",
    "        return torch.zeros(num_layers,batch_size,hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #1 \n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted string : lllooo, epoch: 1, loss: 10.045\n",
      "predicted string : lllloo, epoch: 2, loss: 9.045\n",
      "predicted string : lllllo, epoch: 3, loss: 8.293\n",
      "predicted string : lllllo, epoch: 4, loss: 7.629\n",
      "predicted string : llello, epoch: 5, loss: 6.979\n",
      "predicted string : lhello, epoch: 6, loss: 6.452\n",
      "predicted string : ihello, epoch: 7, loss: 6.066\n",
      "predicted string : ihello, epoch: 8, loss: 5.727\n",
      "predicted string : ihello, epoch: 9, loss: 5.428\n",
      "predicted string : ihello, epoch: 10, loss: 5.184\n",
      "predicted string : ihello, epoch: 11, loss: 4.967\n",
      "predicted string : ihello, epoch: 12, loss: 4.767\n",
      "predicted string : ihello, epoch: 13, loss: 4.608\n",
      "predicted string : ihello, epoch: 14, loss: 4.495\n",
      "predicted string : ihello, epoch: 15, loss: 4.410\n",
      "predicted string : ihello, epoch: 16, loss: 4.324\n",
      "predicted string : ihello, epoch: 17, loss: 4.240\n",
      "predicted string : ihello, epoch: 18, loss: 4.164\n",
      "predicted string : ihello, epoch: 19, loss: 4.095\n",
      "predicted string : ihello, epoch: 20, loss: 4.030\n",
      "predicted string : ihello, epoch: 21, loss: 3.948\n",
      "predicted string : ihello, epoch: 22, loss: 3.841\n",
      "predicted string : ihello, epoch: 23, loss: 3.898\n",
      "predicted string : ihello, epoch: 24, loss: 3.691\n",
      "predicted string : ihello, epoch: 25, loss: 3.649\n",
      "predicted string : ihello, epoch: 26, loss: 3.633\n",
      "predicted string : ihello, epoch: 27, loss: 3.543\n",
      "predicted string : ihello, epoch: 28, loss: 3.482\n",
      "predicted string : ihello, epoch: 29, loss: 3.453\n",
      "predicted string : ihello, epoch: 30, loss: 3.410\n",
      "predicted string : ihello, epoch: 31, loss: 3.371\n",
      "predicted string : ihello, epoch: 32, loss: 3.338\n",
      "predicted string : ihello, epoch: 33, loss: 3.308\n",
      "predicted string : ihello, epoch: 34, loss: 3.284\n",
      "predicted string : ihello, epoch: 35, loss: 3.268\n",
      "predicted string : ihello, epoch: 36, loss: 3.246\n",
      "predicted string : ihello, epoch: 37, loss: 3.224\n",
      "predicted string : ihello, epoch: 38, loss: 3.202\n",
      "predicted string : ihello, epoch: 39, loss: 3.174\n",
      "predicted string : ihello, epoch: 40, loss: 3.144\n",
      "predicted string : ihello, epoch: 41, loss: 3.119\n",
      "predicted string : ihello, epoch: 42, loss: 3.094\n",
      "predicted string : ihello, epoch: 43, loss: 3.070\n",
      "predicted string : ihello, epoch: 44, loss: 3.049\n",
      "predicted string : ihello, epoch: 45, loss: 3.028\n",
      "predicted string : ihello, epoch: 46, loss: 3.006\n",
      "predicted string : ihello, epoch: 47, loss: 2.988\n",
      "predicted string : ihello, epoch: 48, loss: 2.972\n",
      "predicted string : ihello, epoch: 49, loss: 2.957\n",
      "predicted string : ihello, epoch: 50, loss: 2.945\n",
      "predicted string : ihello, epoch: 51, loss: 2.933\n",
      "predicted string : ihello, epoch: 52, loss: 2.922\n",
      "predicted string : ihello, epoch: 53, loss: 2.911\n",
      "predicted string : ihello, epoch: 54, loss: 2.902\n",
      "predicted string : ihello, epoch: 55, loss: 2.893\n",
      "predicted string : ihello, epoch: 56, loss: 2.885\n",
      "predicted string : ihello, epoch: 57, loss: 2.878\n",
      "predicted string : ihello, epoch: 58, loss: 2.872\n",
      "predicted string : ihello, epoch: 59, loss: 2.867\n",
      "predicted string : ihello, epoch: 60, loss: 2.862\n",
      "predicted string : ihello, epoch: 61, loss: 2.858\n",
      "predicted string : ihello, epoch: 62, loss: 2.855\n",
      "predicted string : ihello, epoch: 63, loss: 2.851\n",
      "predicted string : ihello, epoch: 64, loss: 2.848\n",
      "predicted string : ihello, epoch: 65, loss: 2.845\n",
      "predicted string : ihello, epoch: 66, loss: 2.842\n",
      "predicted string : ihello, epoch: 67, loss: 2.839\n",
      "predicted string : ihello, epoch: 68, loss: 2.836\n",
      "predicted string : ihello, epoch: 69, loss: 2.834\n",
      "predicted string : ihello, epoch: 70, loss: 2.832\n",
      "predicted string : ihello, epoch: 71, loss: 2.830\n",
      "predicted string : ihello, epoch: 72, loss: 2.828\n",
      "predicted string : ihello, epoch: 73, loss: 2.826\n",
      "predicted string : ihello, epoch: 74, loss: 2.825\n",
      "predicted string : ihello, epoch: 75, loss: 2.823\n",
      "predicted string : ihello, epoch: 76, loss: 2.821\n",
      "predicted string : ihello, epoch: 77, loss: 2.819\n",
      "predicted string : ihello, epoch: 78, loss: 2.818\n",
      "predicted string : ihello, epoch: 79, loss: 2.816\n",
      "predicted string : ihello, epoch: 80, loss: 2.815\n",
      "predicted string : ihello, epoch: 81, loss: 2.814\n",
      "predicted string : ihello, epoch: 82, loss: 2.812\n",
      "predicted string : ihello, epoch: 83, loss: 2.811\n",
      "predicted string : ihello, epoch: 84, loss: 2.809\n",
      "predicted string : ihello, epoch: 85, loss: 2.808\n",
      "predicted string : ihello, epoch: 86, loss: 2.807\n",
      "predicted string : ihello, epoch: 87, loss: 2.806\n",
      "predicted string : ihello, epoch: 88, loss: 2.804\n",
      "predicted string : ihello, epoch: 89, loss: 2.803\n",
      "predicted string : ihello, epoch: 90, loss: 2.802\n",
      "predicted string : ihello, epoch: 91, loss: 2.801\n",
      "predicted string : ihello, epoch: 92, loss: 2.800\n",
      "predicted string : ihello, epoch: 93, loss: 2.799\n",
      "predicted string : ihello, epoch: 94, loss: 2.798\n",
      "predicted string : ihello, epoch: 95, loss: 2.797\n",
      "predicted string : ihello, epoch: 96, loss: 2.796\n",
      "predicted string : ihello, epoch: 97, loss: 2.795\n",
      "predicted string : ihello, epoch: 98, loss: 2.794\n",
      "predicted string : ihello, epoch: 99, loss: 2.793\n",
      "predicted string : ihello, epoch: 100, loss: 2.792\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): \n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss = 0 \n",
    "    hidden = model.init_hidden() #2 \n",
    "    print(\"predicted string : \",end=\"\") \n",
    "    \n",
    "    for input_,label in zip(inputs, labels): #3\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.view(-1,1)\n",
    "        hidden, output = model(input_, hidden)\n",
    "        output = output.view(-1, num_classes)\n",
    "        val, idx = output.max(1) \n",
    "        \n",
    "        print(idx2char[idx.data[0]],end=\"\") \n",
    "        loss += criterion(output,label) \n",
    "    \n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch+1, loss.item())) \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_classes = 5\n",
    "input_size = 5 # one-hot size \n",
    "hidden_size = 5 #output from the cell. 바로 결과 예측을 위해서 5로 설정 \n",
    "batch_size = 1 # sentence의 개수 (단어개수) \n",
    "sequence_length = 6 # 이번에는 한번에 6개씩 해본다 \n",
    "num_layers = 1 # one layer rnn이다. (아직 안 다룬 개념)\n",
    "\n",
    "idx2char = ['h','i','e','l','o']\n",
    "x_data = [0,1,0,2,3,3] #hihell \n",
    "\n",
    "one_hot_lookup = [[1,0,0,0,0], #h \n",
    "                  [0,1,0,0,0], \n",
    "                  [0,0,1,0,0], \n",
    "                  [0,0,0,1,0], \n",
    "                  [0,0,0,0,1]] \n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] \n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def forward(self, x, hidden): \n",
    "        #input x 를 (batch_size,sequence_length,input_size)로 reshape\n",
    "        x = x.view(batch_size, sequence_length, input_size) \n",
    "        \n",
    "        #Propagate input through RNN \n",
    "        #Input:(batch,seq_len,input_size) \n",
    "        out,hidden = self.rnn(x, hidden) \n",
    "        \n",
    "        #for make sure, output이 N * 5 shape을 따르게 하기 위해서 \n",
    "        out = out.view(-1, num_classes) \n",
    "        \n",
    "        return hidden, out \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for epoch in range(100): \n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss = 0 \n",
    "    hidden = model.init_hidden() #2 \n",
    "    print(\"predicted string : \",end=\"\") \n",
    "    \n",
    "    for input_,label in zip(inputs, labels): #3\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.view(-1,1)\n",
    "        hidden, output = model(input_, hidden)\n",
    "\n",
    "        val, idx = output.max(1) \n",
    "        \n",
    "        print(idx2char[idx.data[0]],end=\"\") \n",
    "        loss += criterion(output,label) \n",
    "    \n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch+1, loss.item())) \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #1 \n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.553\n",
      "Predicted string:  ieelii\n",
      "epoch: 2, loss: 1.390\n",
      "Predicted string:  ilelle\n",
      "epoch: 3, loss: 1.281\n",
      "Predicted string:  ilelll\n",
      "epoch: 4, loss: 1.199\n",
      "Predicted string:  ililll\n",
      "epoch: 5, loss: 1.112\n",
      "Predicted string:  ililll\n",
      "epoch: 6, loss: 1.039\n",
      "Predicted string:  ihilll\n",
      "epoch: 7, loss: 0.974\n",
      "Predicted string:  ihioll\n",
      "epoch: 8, loss: 0.915\n",
      "Predicted string:  ihioll\n",
      "epoch: 9, loss: 0.866\n",
      "Predicted string:  ihioll\n",
      "epoch: 10, loss: 0.824\n",
      "Predicted string:  iheoll\n",
      "epoch: 11, loss: 0.781\n",
      "Predicted string:  iheolo\n",
      "epoch: 12, loss: 0.735\n",
      "Predicted string:  iheolo\n",
      "epoch: 13, loss: 0.685\n",
      "Predicted string:  ihello\n",
      "epoch: 14, loss: 0.642\n",
      "Predicted string:  ihello\n",
      "epoch: 15, loss: 0.614\n",
      "Predicted string:  ihello\n",
      "epoch: 16, loss: 0.586\n",
      "Predicted string:  ihello\n",
      "epoch: 17, loss: 0.568\n",
      "Predicted string:  ihello\n",
      "epoch: 18, loss: 0.561\n",
      "Predicted string:  ihello\n",
      "epoch: 19, loss: 0.554\n",
      "Predicted string:  ihello\n",
      "epoch: 20, loss: 0.544\n",
      "Predicted string:  ihello\n",
      "epoch: 21, loss: 0.535\n",
      "Predicted string:  ihello\n",
      "epoch: 22, loss: 0.528\n",
      "Predicted string:  ihello\n",
      "epoch: 23, loss: 0.522\n",
      "Predicted string:  ihello\n",
      "epoch: 24, loss: 0.517\n",
      "Predicted string:  ihello\n",
      "epoch: 25, loss: 0.511\n",
      "Predicted string:  ihello\n",
      "epoch: 26, loss: 0.505\n",
      "Predicted string:  ihello\n",
      "epoch: 27, loss: 0.505\n",
      "Predicted string:  ihello\n",
      "epoch: 28, loss: 0.502\n",
      "Predicted string:  ihello\n",
      "epoch: 29, loss: 0.496\n",
      "Predicted string:  ihello\n",
      "epoch: 30, loss: 0.495\n",
      "Predicted string:  ihello\n",
      "epoch: 31, loss: 0.494\n",
      "Predicted string:  ihello\n",
      "epoch: 32, loss: 0.492\n",
      "Predicted string:  ihello\n",
      "epoch: 33, loss: 0.489\n",
      "Predicted string:  ihello\n",
      "epoch: 34, loss: 0.487\n",
      "Predicted string:  ihello\n",
      "epoch: 35, loss: 0.489\n",
      "Predicted string:  ihello\n",
      "epoch: 36, loss: 0.485\n",
      "Predicted string:  ihello\n",
      "epoch: 37, loss: 0.483\n",
      "Predicted string:  ihello\n",
      "epoch: 38, loss: 0.483\n",
      "Predicted string:  ihello\n",
      "epoch: 39, loss: 0.482\n",
      "Predicted string:  ihello\n",
      "epoch: 40, loss: 0.480\n",
      "Predicted string:  ihello\n",
      "epoch: 41, loss: 0.480\n",
      "Predicted string:  ihello\n",
      "epoch: 42, loss: 0.480\n",
      "Predicted string:  ihello\n",
      "epoch: 43, loss: 0.478\n",
      "Predicted string:  ihello\n",
      "epoch: 44, loss: 0.478\n",
      "Predicted string:  ihello\n",
      "epoch: 45, loss: 0.477\n",
      "Predicted string:  ihello\n",
      "epoch: 46, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 47, loss: 0.475\n",
      "Predicted string:  ihello\n",
      "epoch: 48, loss: 0.475\n",
      "Predicted string:  ihello\n",
      "epoch: 49, loss: 0.474\n",
      "Predicted string:  ihello\n",
      "epoch: 50, loss: 0.474\n",
      "Predicted string:  ihello\n",
      "epoch: 51, loss: 0.474\n",
      "Predicted string:  ihello\n",
      "epoch: 52, loss: 0.473\n",
      "Predicted string:  ihello\n",
      "epoch: 53, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 54, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 55, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 56, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 57, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 58, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 59, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 60, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 61, loss: 0.470\n",
      "Predicted string:  ihello\n",
      "epoch: 62, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 63, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 64, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 65, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 66, loss: 0.468\n",
      "Predicted string:  ihello\n",
      "epoch: 67, loss: 0.468\n",
      "Predicted string:  ihello\n",
      "epoch: 68, loss: 0.468\n",
      "Predicted string:  ihello\n",
      "epoch: 69, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 70, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 71, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 72, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 73, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 74, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 75, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 76, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 77, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 78, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 79, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 80, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 81, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 82, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 83, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 84, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 85, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 86, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 87, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 88, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 89, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 90, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 91, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 92, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 93, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 94, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 95, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 96, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 97, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 98, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 99, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 100, loss: 0.462\n",
      "Predicted string:  ihello\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100): \n",
    "    optimizer.zero_grad() \n",
    "    loss = 0 \n",
    "    hidden = model.init_hidden() \n",
    "    \n",
    "    # print(\"predicted string : \",end=\"\") \n",
    "    inputs = inputs.to(device) \n",
    "    labels = labels.to(device) \n",
    "    \n",
    "    _, outputs = model(inputs,hidden) \n",
    "    \n",
    "    outputs = outputs.view(-1, 5)\n",
    "    loss = criterion(outputs, labels.squeeze()) \n",
    "    _, idx = outputs.max(1) \n",
    "\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()] \n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch+1,loss.item())) \n",
    "    print(\"Predicted string: \", ''.join(result_str)) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
