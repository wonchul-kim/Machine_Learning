{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  An LSTM for Part-of-Speech Tagging\n",
    "\n",
    "let our input sentence be $w_1, ..., w_M, where w_i \\in V$, our vocab. Also, let $T$ be our tag set, and $y_i$ the tag of word $w_i$. Denote our prediction of the tag of word $w_i$ by $y^i$.\n",
    "\n",
    "This is a structure prediction, model, where our output is a sequence $y^1, ..., y^M, where y^i \\in T$.\n",
    "\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden state at timestep $i$ as $h_i$. Also, assign each tag a unique index (like how we had word_to_$i_x$ in the word embeddings section). Then our prediction rule for $y^i$ is\n",
    "\n",
    "$$ y^i = argmax_j (\\log Softmax(Ah_i+b))_j$$\n",
    "\n",
    "> A, b는 linear layer의 weights?\n",
    "\n",
    "That is, take the log softmax of the affine map of the hidden state, and the predicted tag is the tag that has the maximum value in this vector. Note this implies immediately that the dimensionality of the target space of $A$ is $|T|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data:\n",
    "def word2idx(word, to_ix):\n",
    "    idxs = [to_ix[w] for w in word]\n",
    "\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']),\n",
       " (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "dict_word2idx = {}\n",
    "\n",
    "for sentence, tag in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in dict_word2idx:\n",
    "            dict_word2idx[word] = len(dict_word2idx)\n",
    "print(dict_word2idx)\n",
    "\n",
    "dict_tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> how to decide the dim?\n",
    "\n",
    "> does both of tem have to be same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeddings.view(len(sentence), 1, -1))\n",
    "        tag_space = self.fc(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(EMBEDDING_DIM, HIDDEN_DIM, len(dict_word2idx), len(dict_tag2idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[-0.0407, -3.7853, -4.0645],\n",
      "        [-4.1793, -0.0685, -2.9776],\n",
      "        [-3.9639, -3.1709, -0.0629],\n",
      "        [-0.0136, -4.6197, -5.6138],\n",
      "        [-4.5342, -0.0339, -3.7879]])\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "------------------------------------------------------------\n",
      "tensor([[ 0.3923, -0.0048, -1.3002, -1.2137, -0.6849, -0.1871],\n",
      "        [-0.4928, -0.0336, -0.4821,  1.0056, -1.3088,  0.1777],\n",
      "        [ 0.6634, -0.7987,  2.5142,  1.4382,  2.5683,  0.7818],\n",
      "        [ 1.0874,  1.6349, -0.6058,  1.3048,  0.0794, -0.2434],\n",
      "        [ 0.6373, -0.6516,  0.2432,  0.1030, -2.0268, -1.4972]])\n",
      "tensor(0)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(1)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = word2idx(training_data[0][0], dict_word2idx)\n",
    "    print(inputs)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "    for i in range(len(tag_scores)):\n",
    "        print(np.argmax(tag_scores[i]))\n",
    "        \n",
    "    print(\"------------------------------------------------------------\")\n",
    "    embeddings = nn.Embedding(6, 6)\n",
    "    embeds = embeddings(inputs)\n",
    "    print(embeds)\n",
    "    for i in range(len(embeds)):\n",
    "        print(np.argmax(embeds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):  \n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = word2idx(sentence, dict_word2idx)\n",
    "        targets = word2idx(tags, dict_tag2idx)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0407, -3.7853, -4.0645],\n",
      "        [-4.1793, -0.0685, -2.9776],\n",
      "        [-3.9639, -3.1709, -0.0629],\n",
      "        [-0.0136, -4.6197, -5.6138],\n",
      "        [-4.5342, -0.0339, -3.7879]])\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)\n",
    "    for i in range(len(tag_scores)):\n",
    "        print(np.argmax(tag_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
