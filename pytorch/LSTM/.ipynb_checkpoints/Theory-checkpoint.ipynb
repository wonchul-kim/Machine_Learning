{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs은 sequential data에 대해서 학습이 가능한다는 점에서 일반적인 DNNs보다 장점이 이 있지만, vanishing/expolding gradients에 대한 문제점이 여전히 발생하고 있습니다. 그리고 이를 해결하려고 한 거이 LSTM networks입니다. vanishing/exploding gradients 문제는 학습하는데에 있어서 back-propagation할 때 나타나는 문제점으로 특히나 deeper layer로 갈수록 심해집니다. 이는 back-propagation의 계산이 chain-rule에 의해서 이루어지기 때문에 어쩔 수 없이 gradient가 exponentially vanishing/exploding하기 때문입니다. 그리고 gradient가 너무 작으면 weights를 업데이트하는데에 문제가 발생하고, 반면에 너무 크면 불안정하게 학습이 진행되게 됩니다. 따라서, LSTM은 **long-term memory**라는 module을 통해서 이전의 gradient를 보존함으로서 좀더 긴 sequence 또는 long-term을 가지는 데이터도 학습이 가능합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are LSTMs\n",
    "\n",
    "While LSTMs are a kind of RNN and function similarly to traditional RNNs, its Gating mechanism is what sets it apart. This feature addresses the “short-term memory” problem of RNNs.\n",
    "\n",
    "<img src='./imgs/rnn_lstm.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>\n",
    "\n",
    "'Short-term memory' is especially important in the majority of Natural Language Processing (NLP) or time-series and sequential tasks. For example, let’s say we have a network generating text based on some input given to us. At the start of the text, it is mentioned that the author has a “**dog named Cliff**”. After a few other sentences where there is no mention of a pet or dog, the author brings up his pet again, and the model has to generate the next word to \"However, Cliff, my pet $__$\". As the word pet appeared right before the blank, a RNN can deduce that the next word will likely be an animal that can be kept as a pet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./imgs/exam1.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>\n",
    "\n",
    "However, due to the short-term memory, the typical RNN will only be able to use the contextual information from the text that appeared in the last few sentences - which is not useful at all. The RNN has no clue as to what animal the pet might be as the relevant information from the start of the text has already been lost.\n",
    "\n",
    "On the other hand, the LSTM can retain the earlier information that the author has a pet dog, and this will aid the model in choosing \"the dog\" when it comes to generating the text at that point due to the contextual information from a much earlier time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./imgs/exam2.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner workings of the LSTM\n",
    "\n",
    "In the normal RNN cell, the input at a time-step and the hidden state from the previous time step is passed through a ***tanh*** activation function to obtain a new hidden state and output.\n",
    "\n",
    "<img src='./imgs/inner_arch.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, at each time step, the LSTM cell takes in 3 different pieces of information \n",
    "\n",
    "* the current input data\n",
    "* the short-term memory from the previous cell (similar to hidden states in RNNs) \n",
    "* lastly the long-term memory\n",
    "\n",
    "The short-term memory is commonly referred to as the hidden state, and the long-term memory is usually known as the cell state.\n",
    "\n",
    "The cell then uses gates to regulate the information to be kept or discarded at each time step before passing on the long-term and short-term information to the next cell.\n",
    "\n",
    "> How does the cell decide which information is usefull ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gates are called **the Input Gate, the Forget Gate, and the Output Gate**. \n",
    "\n",
    "<img src='./imgs/gates.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Gate\n",
    "\n",
    "<img src='./imgs/input_gate.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>\n",
    "\n",
    "The input gate decides what new information will be stored in the long-term memory. It only works with the information from the current input and the short-term memory from the previous time step. \n",
    "\n",
    "Mathematically, this is achieved using 2 layers. The first layer can be seen as the filter which selects what information can pass through it and what information to be discarded. To create this layer, we pass the short-term memory and current input into a sigmoid function. The sigmoid function will transform the values to be between 0 and 1, with 0 indicating that part of the information is unimportant, whereas 1 indicates that the information will be used. As the layer is being trained through back-propagation, the weights in the sigmoid function will be updated such that it learns to only let the useful pass through while discarding the less critical features.\n",
    "\n",
    "$$ i_1 = \\alpha(W_{i_1}\\times (H_{t-1}, x_t) + bias_{i_1})$$\n",
    "\n",
    "> $(H_{t-1}, x_t)$ is concatenation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layer takes the short term memory and current input as well and passes it through an activation function, usually the $tanh$ function, to regulate the network.\n",
    "\n",
    "$$ i_2 = tanh(W_{i_2}\\times (H_{t-1}, x_t) + bias_{i_2})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs from these 2 layers are then multiplied, and the final outcome represents the information to be kept in the long-term memory and used as the output.\n",
    "\n",
    "$$ i_{input} = i_1\\times i_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forget Gate\n",
    "\n",
    "<img src='./imgs/forget_gate.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>\n",
    "\n",
    "The forget gate decides which information from the long-term memory should be kept or discarded. This is done by multiplying the incoming long-term memory by a forget vector generated by the current input and incoming short-term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forget vector($f$) is also a selective filter layer. To obtain the forget vector, the short-term memory, and current input is passed through a sigmoid function. The vector will be made up of 0s and 1s and will be multiplied with the long-term memory to choose which parts of the long-term memory to retain.\n",
    "\n",
    "$$ f = \\alpha(W_{forget}\\times (H_{t-1}, x_t) + bias_{forget})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs from the Input gate and the Forget gate will undergo a pointwise addition to give a new version of the long-term memory, which will be passed on to the next cell. This new long-term memory will also be used in the final gate, the Output gate.\n",
    "\n",
    "$$ C_t = C_{t-1} \\times f + i_{input}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Gate\n",
    "\n",
    "<img src='./imgs/output_gate.JPG'>\n",
    "<center><span style=\"color:gray\"><em>from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/</em></span></center>\n",
    "\n",
    "The output gate will take the current input, the previous short-term memory, and the newly computed long-term memory to produce the new short-term memory/hidden state which will be passed on to the cell in the next time step. The output of the current time step can also be drawn from this hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the previous short-term memory and current input will be passed into a sigmoid function to create the final filter. Then, we put the new long-term memory through an activation $tanh$ function. The output from these 2 processes will be multiplied to produce the new short-term memory.\n",
    "\n",
    "$$ O_1 = \\alpha(W_{output_1}\\times (H_{t-1}, x_t) + bias_{output_1})$$\n",
    "\n",
    "\n",
    "$$ O_2 = tanh(W_{output_2}\\times C_t + bias_{output_2})$$\n",
    "\n",
    "$$ H_t, O_t = O_1\\times O_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
