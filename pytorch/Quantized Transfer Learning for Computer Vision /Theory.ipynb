{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Quantization\n",
    "\n",
    "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. A quantized model executes some or all of the operations on tensors with integers rather than floating point values. This allows for a more compact model representation and ***the use of high performance vectorized operations on many hardware platforms.*** PyTorch supports INT8 quantization compared to typical FP32 models allowing for a 4x reduction in the model size and a 4x reduction in memory bandwidth requirements. Hardware support for INT8 computations is typically 2 to 4 times faster compared to FP32 compute. **Quantization is primarily a technique to speed up inference and only the forward pass is supported for quantized operators.**\n",
    "\n",
    "> float -> integer가 속도가 더 빠른 건 알겠지만 왜 high performance vectorized operations를 해주지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports multiple approaches to quantizing a deep learning model. In most cases the model is trained in FP32 and then the model is converted to INT8. In addition, PyTorch also supports ***quantization aware training, which models quantization errors in both the forward and backward passes using fake-quantization modules.*** Note that the entire computation is carried out in floating point. At the end of quantization aware training, PyTorch provides conversion functions to convert the trained model into lower precision.\n",
    "\n",
    "> quantization aware training?\n",
    "\n",
    "At lower level, PyTorch provides a way to represent quantized tensors and perform operations with them. They can be used to directly construct models that perform all or part of the computation in lower precision. Higher-level APIs are provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss.\n",
    "\n",
    "Today, PyTorch supports the following backends for running quantized operators efficiently:\n",
    "\n",
    "* x86 CPUs with AVX2 support or higher (without AVX2 some operations have inefficient implementations)\n",
    "\n",
    "* ARM CPUs (typically found in mobile/embedded devices)\n",
    "\n",
    "The corresponding implementation is chosen automatically based on the PyTorch build mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
